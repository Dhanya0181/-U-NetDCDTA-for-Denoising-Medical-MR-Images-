{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YC2u3f9Hco4C",
        "outputId": "6b677b44-ef8f-4f42-a9d4-7a712d5050c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N23MfN7DSB6F"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "#import timm\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as  nn\n",
        "\n",
        "def preprocess_input(x, mean=None, std=None, input_space=\"RGB\", input_range=None, **kwargs):\n",
        "\n",
        "    if input_space == \"BGR\":\n",
        "        x = x[..., ::-1].copy()\n",
        "\n",
        "    if input_range is not None:\n",
        "        if x.max() > 1 and input_range[1] == 1:\n",
        "            x = x / 255.0\n",
        "\n",
        "    if mean is not None:\n",
        "        mean = np.array(mean)\n",
        "        x = x - mean\n",
        "\n",
        "    if std is not None:\n",
        "        std = np.array(std)\n",
        "        x = x / std\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def patch_first_conv(model, new_in_channels, default_in_channels=3, pretrained=True):\n",
        "\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, nn.Conv2d) and module.in_channels == default_in_channels:\n",
        "            break\n",
        "\n",
        "    weight = module.weight.detach()\n",
        "    module.in_channels = new_in_channels\n",
        "\n",
        "    if not pretrained:\n",
        "        module.weight = nn.parameter.Parameter( torch.Tensor(module.out_channels,\n",
        "                         new_in_channels // module.groups, *module.kernel_size))\n",
        "        module.reset_parameters()\n",
        "\n",
        "    elif new_in_channels == 1:\n",
        "        new_weight = weight.sum(1, keepdim=True)\n",
        "        module.weight = nn.parameter.Parameter(new_weight)\n",
        "\n",
        "    else:\n",
        "        new_weight = torch.Tensor(module.out_channels, new_in_channels // module.groups, *module.kernel_size)\n",
        "\n",
        "        for i in range(new_in_channels):\n",
        "            new_weight[:, i] = weight[:, i % default_in_channels]\n",
        "\n",
        "        new_weight = new_weight * (default_in_channels / new_in_channels)\n",
        "        module.weight = nn.parameter.Parameter(new_weight)\n",
        "\n",
        "\n",
        "def replace_strides_with_dilation(module, dilation_rate):\n",
        "\n",
        "    for mod in module.modules():\n",
        "        if isinstance(mod, nn.Conv2d):\n",
        "            mod.stride = (1, 1)\n",
        "            mod.dilation = (dilation_rate, dilation_rate)\n",
        "            kh, kw = mod.kernel_size\n",
        "            mod.padding = ((kh // 2) * dilation_rate, (kh // 2) * dilation_rate)\n",
        "\n",
        "            if hasattr(mod, \"static_padding\"):\n",
        "                mod.static_padding = nn.Identity()\n",
        "\n",
        "\n",
        "class EncoderMixin:\n",
        "\n",
        "    _output_stride = 32\n",
        "\n",
        "\n",
        "    @property\n",
        "    def out_channels(self):\n",
        "        return self._out_channels[: self._depth + 1]\n",
        "\n",
        "    @property\n",
        "    def output_stride(self):\n",
        "        return min(self._output_stride, 2 ** self._depth)\n",
        "\n",
        "    def set_in_channels(self, in_channels, pretrained=True):\n",
        "        if in_channels == 3:\n",
        "            return\n",
        "\n",
        "        self._in_channels = in_channels\n",
        "        if self._out_channels[0] == 3:\n",
        "            self._out_channels = tuple([in_channels] + list(self._out_channels)[1:])\n",
        "\n",
        "        patch_first_conv(model=self, new_in_channels=in_channels, pretrained=pretrained)\n",
        "\n",
        "    def get_stages(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def make_dilated(self, output_stride):\n",
        "\n",
        "        if output_stride == 16:\n",
        "            stage_list = [\n",
        "                5,\n",
        "            ]\n",
        "            dilation_list = [\n",
        "                2,\n",
        "            ]\n",
        "\n",
        "        elif output_stride == 8:\n",
        "            stage_list = [4, 5]\n",
        "            dilation_list = [2, 4]\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Output stride should be 16 or 8, got {}.\".format(output_stride))\n",
        "\n",
        "        self._output_stride = output_stride\n",
        "\n",
        "        stages = self.get_stages()\n",
        "        for stage_indx, dilation_rate in zip(stage_list, dilation_list):\n",
        "            replace_strides_with_dilation(\n",
        "                module=stages[stage_indx],\n",
        "                dilation_rate=dilation_rate,)\n",
        "\n",
        "\n",
        "class TimmUniversalEncoder(nn.Module):\n",
        "    def __init__(self, name, pretrained=True, in_channels=3, depth=5, output_stride=32):\n",
        "        super().__init__()\n",
        "        kwargs = dict( in_chans=in_channels,   features_only=True,  output_stride=output_stride,\n",
        "            pretrained=pretrained,   out_indices=tuple(range(depth)), )\n",
        "\n",
        "        if output_stride == 32:\n",
        "            kwargs.pop(\"output_stride\")\n",
        "\n",
        "        self.model = timm.create_model(name, **kwargs)\n",
        "        self._in_channels = in_channels\n",
        "        self._out_channels = [in_channels,] + self.model.feature_info.channels()\n",
        "        self._depth = depth\n",
        "        self._output_stride = output_stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.model(x)\n",
        "        features = [ x, ] + features\n",
        "        return features\n",
        "\n",
        "    @property\n",
        "    def out_channels(self):\n",
        "        return self._out_channels\n",
        "\n",
        "    @property\n",
        "    def output_stride(self):\n",
        "        return min(self._output_stride, 2 ** self._depth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whspgSTtbPxr",
        "outputId": "fde4a145-24ce-4822-c1c2-94fa465fa5aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm\n",
            "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.2+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0)\n",
            "Collecting huggingface-hub (from timm)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors (from timm)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Installing collected packages: safetensors, huggingface-hub, timm\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 timm-0.9.2\n"
          ]
        }
      ],
      "source": [
        "pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vW4YTL9LSUgN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as  nn\n",
        "\n",
        "\n",
        "#from .utils import EncoderMixin\n",
        "from timm.models.resnet import ResNet\n",
        "from timm.models.resnet import Bottleneck\n",
        "\n",
        "\n",
        "\n",
        "class ResNestEncoder(ResNet, EncoderMixin):\n",
        "    def __init__(self, out_channels, depth=5, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self._depth = depth\n",
        "        self._out_channels = out_channels\n",
        "        self._in_channels = 3\n",
        "\n",
        "        del self.fc\n",
        "        del self.global_pool\n",
        "\n",
        "    def get_stages(self):\n",
        "\n",
        "        return [\n",
        "            nn.Identity(), nn.Sequential(self.conv1, self.bn1, self.act1),\n",
        "            nn.Sequential(self.maxpool, self.layer1), self.layer2, self.layer3, self.layer4\n",
        "        ]\n",
        "\n",
        "    def make_dilated(self, *args, **kwargs):\n",
        "        raise ValueError(\"ResNest encoders do not support dilated mode\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        stages = self.get_stages()\n",
        "        features = [ ]\n",
        "\n",
        "        for i in range(self._depth + 1):\n",
        "            x = stages[i](x)\n",
        "            features.append(x)\n",
        "        return features\n",
        "\n",
        "    def load_state_dict(self, state_dict, **kwargs):\n",
        "        state_dict.pop(\"fc.bias\", None)\n",
        "        state_dict.pop(\"fc.weight\", None)\n",
        "        super().load_state_dict(state_dict, **kwargs)\n",
        "\n",
        "\n",
        "resnest_weights = {\n",
        "    \"timm-resnest14d\": {\n",
        "        \"imagenet\": \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/gluon_resnest14-9c8fe254.pth\",  # noqa\n",
        "    },\n",
        "    \"timm-resnest26d\": {\n",
        "        \"imagenet\": \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/gluon_resnest26-50eb607c.pth\",  # noqa\n",
        "    },\n",
        "    \"timm-resnest50d\": {\n",
        "        \"imagenet\": \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-resnest/resnest50-528c19ca.pth\",  # noqa\n",
        "    },\n",
        "    \"timm-resnest101e\": {\n",
        "        \"imagenet\": \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-resnest/resnest101-22405ba7.pth\",  # noqa\n",
        "    },\n",
        "    \"timm-resnest200e\": {\n",
        "        \"imagenet\": \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-resnest/resnest200-75117900.pth\",  # noqa\n",
        "    },\n",
        "    \"timm-resnest269e\": {\n",
        "        \"imagenet\": \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-resnest/resnest269-0cc87c48.pth\",  # noqa\n",
        "    },\n",
        "    \"timm-resnest50d_4s2x40d\": {\n",
        "        \"imagenet\": \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-resnest/resnest50_fast_4s2x40d-41d14ed0.pth\",  # noqa\n",
        "    },\n",
        "    \"timm-resnest50d_1s4x24d\": {\n",
        "        \"imagenet\": \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-resnest/resnest50_fast_1s4x24d-d4a4f76f.pth\",  # noqa\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "pretrained_settings = {}\n",
        "for model_name, sources in resnest_weights.items():\n",
        "    pretrained_settings[model_name] = {}\n",
        "    for source_name, source_url in sources.items():\n",
        "        pretrained_settings[model_name][source_name] = {\n",
        "            \"url\": source_url,\n",
        "            \"input_size\": [3, 224, 224],\n",
        "            \"input_range\": [0, 1],\n",
        "            \"mean\": [0.485, 0.456, 0.406],\n",
        "            \"std\": [0.229, 0.224, 0.225],\n",
        "            \"num_classes\": 1000,\n",
        "        }\n",
        "\n",
        "\n",
        "timm_resnest_encoders = {\n",
        "    \"timm-resnest14d\": {\n",
        "        \"encoder\": ResNestEncoder,\n",
        "        \"pretrained_settings\": pretrained_settings[\"timm-resnest14d\"],\n",
        "        \"params\": {\n",
        "            \"out_channels\": (3, 64, 256, 512, 1024, 2048),\n",
        "            \"block\": Bottleneck,\n",
        "            \"layers\": [1, 1, 1, 1],\n",
        "            \"stem_type\": \"deep\",\n",
        "            \"stem_width\": 32,\n",
        "            \"avg_down\": True,\n",
        "            \"base_width\": 64,\n",
        "            \"cardinality\": 1,\n",
        "            \"block_args\": {\"radix\": 2, \"avd\": True, \"avd_first\": False},\n",
        "        },\n",
        "    },\n",
        "\n",
        "    \"timm-resnest26d\": {\n",
        "        \"encoder\": ResNestEncoder,\n",
        "        \"pretrained_settings\": pretrained_settings[\"timm-resnest26d\"],\n",
        "        \"params\": {\n",
        "            \"out_channels\": (3, 64, 256, 512, 1024, 2048),\n",
        "            \"block\": Bottleneck,\n",
        "            \"layers\": [2, 2, 2, 2],\n",
        "            \"stem_type\": \"deep\",\n",
        "            \"stem_width\": 32,\n",
        "            \"avg_down\": True,\n",
        "            \"base_width\": 64,\n",
        "            \"cardinality\": 1,\n",
        "            \"block_args\": {\"radix\": 2, \"avd\": True, \"avd_first\": False},\n",
        "        },\n",
        "    },\n",
        "\n",
        "    \"timm-resnest50d\": {\n",
        "        \"encoder\": ResNestEncoder,\n",
        "        \"pretrained_settings\": pretrained_settings[\"timm-resnest50d\"],\n",
        "        \"params\": {\n",
        "            \"out_channels\": (3, 64, 256, 512, 1024, 2048),\n",
        "            \"block\": Bottleneck,\n",
        "            \"layers\": [3, 4, 6, 3],\n",
        "            \"stem_type\": \"deep\",\n",
        "            \"stem_width\": 32,\n",
        "            \"avg_down\": True,\n",
        "            \"base_width\": 64,\n",
        "            \"cardinality\": 1,\n",
        "            \"block_args\": {\"radix\": 2, \"avd\": True, \"avd_first\": False},\n",
        "        },\n",
        "    },\n",
        "\n",
        "    \"timm-resnest101e\": {\n",
        "        \"encoder\": ResNestEncoder,\n",
        "        \"pretrained_settings\": pretrained_settings[\"timm-resnest101e\"],\n",
        "        \"params\": {\n",
        "            \"out_channels\": (3, 128, 256, 512, 1024, 2048),\n",
        "            \"block\": Bottleneck,\n",
        "            \"layers\": [3, 4, 23, 3],\n",
        "            \"stem_type\": \"deep\",\n",
        "            \"stem_width\": 64,\n",
        "            \"avg_down\": True,\n",
        "            \"base_width\": 64,\n",
        "            \"cardinality\": 1,\n",
        "            \"block_args\": {\"radix\": 2, \"avd\": True, \"avd_first\": False},\n",
        "        },\n",
        "    },\n",
        "\n",
        "     \"timm-resnest200e\": {\n",
        "        \"encoder\": ResNestEncoder,\n",
        "        \"pretrained_settings\": pretrained_settings[\"timm-resnest200e\"],\n",
        "        \"params\": {\n",
        "            \"out_channels\": (3, 128, 256, 512, 1024, 2048),\n",
        "            \"block\": Bottleneck,\n",
        "            \"layers\": [3, 24, 36, 3],\n",
        "            \"stem_type\": \"deep\",\n",
        "            \"stem_width\": 64,\n",
        "            \"avg_down\": True,\n",
        "            \"base_width\": 64,\n",
        "            \"cardinality\": 1,\n",
        "            \"block_args\": {\"radix\": 2, \"avd\": True, \"avd_first\": False},\n",
        "        },\n",
        "    },\n",
        "\n",
        "    \"timm-resnest269e\": {\n",
        "        \"encoder\": ResNestEncoder,\n",
        "        \"pretrained_settings\": pretrained_settings[\"timm-resnest269e\"],\n",
        "        \"params\": {\n",
        "            \"out_channels\": (3, 128, 256, 512, 1024, 2048),\n",
        "            \"block\": Bottleneck,\n",
        "            \"layers\": [3, 30, 48, 8],\n",
        "            \"stem_type\": \"deep\",\n",
        "            \"stem_width\": 64,\n",
        "            \"avg_down\": True,\n",
        "            \"base_width\": 64,\n",
        "            \"cardinality\": 1,\n",
        "            \"block_args\": {\"radix\": 2, \"avd\": True, \"avd_first\": False},\n",
        "        },\n",
        "    },\n",
        "\n",
        "    \"timm-resnest50d_4s2x40d\": {\n",
        "        \"encoder\": ResNestEncoder,\n",
        "        \"pretrained_settings\": pretrained_settings[\"timm-resnest50d_4s2x40d\"],\n",
        "        \"params\": {\n",
        "            \"out_channels\": (3, 64, 256, 512, 1024, 2048),\n",
        "            \"block\": Bottleneck,\n",
        "            \"layers\": [3, 4, 6, 3],\n",
        "            \"stem_type\": \"deep\",\n",
        "            \"stem_width\": 32,\n",
        "            \"avg_down\": True,\n",
        "            \"base_width\": 40,\n",
        "            \"cardinality\": 2,\n",
        "            \"block_args\": {\"radix\": 4, \"avd\": True, \"avd_first\": True},\n",
        "        },\n",
        "    },\n",
        "\n",
        "    \"timm-resnest50d_1s4x24d\": {\n",
        "        \"encoder\": ResNestEncoder,\n",
        "        \"pretrained_settings\": pretrained_settings[\"timm-resnest50d_1s4x24d\"],\n",
        "        \"params\": {\n",
        "            \"out_channels\": (3, 64, 256, 512, 1024, 2048),\n",
        "            \"block\": Bottleneck,\n",
        "            \"layers\": [3, 4, 6, 3],\n",
        "            \"stem_type\": \"deep\",\n",
        "            \"stem_width\": 32,\n",
        "            \"avg_down\": True,\n",
        "            \"base_width\": 24,\n",
        "            \"cardinality\": 4,\n",
        "            \"block_args\": {\"radix\": 1, \"avd\": True, \"avd_first\": True},\n",
        "        },\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gQWFzM6SB0D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torchvision.models.resnet import ResNet\n",
        "from torchvision.models.resnet import Bottleneck\n",
        "from torchvision.models.resnet import BasicBlock\n",
        "#from pretrainedmodels.models.torchvision_models import pretrained_settings\n",
        "\n",
        "#from .utils import EncoderMixin\n",
        "\n",
        "\n",
        "class ResNetEncoder(ResNet, EncoderMixin):\n",
        "    def __init__(self, out_channels, depth=5, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self._depth = depth\n",
        "        self._out_channels = out_channels\n",
        "        self._in_channels = 3\n",
        "\n",
        "        del self.fc\n",
        "        del self.avgpool\n",
        "\n",
        "    def get_stages(self):\n",
        "        return [\n",
        "            nn.Identity(),\n",
        "            nn.Sequential(self.conv1, self.bn1, self.relu),\n",
        "            nn.Sequential(self.maxpool, self.layer1),\n",
        "            self.layer2,\n",
        "            self.layer3,\n",
        "            self.layer4,\n",
        "        ]\n",
        "\n",
        "    def forward(self, x):\n",
        "        stages = self.get_stages()\n",
        "\n",
        "        features = [ ]\n",
        "        for i in range(self._depth + 1):\n",
        "            x = stages[i](x)\n",
        "            features.append(x)\n",
        "        return features\n",
        "\n",
        "    def load_state_dict(self, state_dict, **kwargs):\n",
        "        state_dict.pop(\"fc.bias\", None)\n",
        "        state_dict.pop(\"fc.weight\", None)\n",
        "        super().load_state_dict(state_dict, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "new_settings = {\n",
        "    \"resnet18\": {\n",
        "        \"ssl\": \"https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnet18-d92f0530.pth\",  # noqa\n",
        "        \"swsl\": \"https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnet18-118f1556.pth\",  # noqa\n",
        "    },\n",
        "\n",
        "    \"resnet50\": {\n",
        "        \"ssl\": \"https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnet50-08389792.pth\",  # noqa\n",
        "        \"swsl\": \"https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnet50-16a12f1b.pth\",  # noqa\n",
        "    },\n",
        "\n",
        "     \"resnext50_32x4d\": {\n",
        "        \"imagenet\": \"https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\",\n",
        "        \"ssl\": \"https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnext50_32x4-ddb3e555.pth\",  # noqa\n",
        "        \"swsl\": \"https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnext50_32x4-72679e44.pth\",  # noqa\n",
        "    },\n",
        "    \"resnext101_32x4d\": {\n",
        "        \"ssl\": \"https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnext101_32x4-dc43570a.pth\",  # noqa\n",
        "        \"swsl\": \"https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnext101_32x4-3f87e46b.pth\",  # noqa\n",
        "    },\n",
        "    \"resnext101_32x8d\": {\n",
        "        \"imagenet\": \"https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\",\n",
        "        \"instagram\": \"https://download.pytorch.org/models/ig_resnext101_32x8-c38310e5.pth\",\n",
        "        \"ssl\": \"https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnext101_32x8-2cfe2f8b.pth\",  # noqa\n",
        "        \"swsl\": \"https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnext101_32x8-b4712904.pth\",  # noqa\n",
        "    },\n",
        "    \"resnext101_32x16d\": {\n",
        "        \"instagram\": \"https://download.pytorch.org/models/ig_resnext101_32x16-c6f796b0.pth\",\n",
        "        \"ssl\": \"https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnext101_32x16-15fffa57.pth\",  # noqa\n",
        "        \"swsl\": \"https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnext101_32x16-f3559a9c.pth\",  # noqa\n",
        "    },\n",
        "    \"resnext101_32x32d\": {\n",
        "        \"instagram\": \"https://download.pytorch.org/models/ig_resnext101_32x32-e4b90b00.pth\",\n",
        "    },\n",
        "    \"resnext101_32x48d\": {\n",
        "        \"instagram\": \"https://download.pytorch.org/models/ig_resnext101_32x48-3e41cc8a.pth\",\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "pretrained_settings = deepcopy(pretrained_settings)\n",
        "\n",
        "for model_name, sources in new_settings.items():\n",
        "    if model_name not in pretrained_settings:\n",
        "        pretrained_settings[model_name] = {}\n",
        "\n",
        "    for source_name, source_url in sources.items():\n",
        "        pretrained_settings[model_name][source_name] = {\n",
        "            \"url\": source_url,\n",
        "            \"input_size\": [3, 224, 224],\n",
        "            \"input_range\": [0, 1],\n",
        "            \"mean\": [0.485, 0.456, 0.406],\n",
        "            \"std\": [0.229, 0.224, 0.225],\n",
        "            \"num_classes\": 1000,\n",
        "        }\n",
        "\n",
        "resnet_encoders = {\n",
        "    \"resnet18\": {\n",
        "        \"encoder\": ResNetEncoder,\n",
        "        \"pretrained_settings\": pretrained_settings[\"resnet18\"],\n",
        "        \"params\": {\n",
        "            \"out_channels\": (3, 64, 64, 128, 256, 512),\n",
        "            \"block\": BasicBlock,\n",
        "            \"layers\": [2, 2, 2, 2],\n",
        "        },\n",
        "    },\n",
        "    #\"resnet34\": {\n",
        "        #\"encoder\": ResNetEncoder,\n",
        "        #\"pretrained_settings\": pretrained_settings[\"resnet34\"],\n",
        "        #\"params\": {\n",
        "            #\"out_channels\": (3, 64, 64, 128, 256, 512),\n",
        "            #\"block\": BasicBlock,\n",
        "            #\"layers\": [3, 4, 6, 3],\n",
        "        #},\n",
        "    #},\n",
        "    \"resnet50\": {\n",
        "        \"encoder\": ResNetEncoder,\n",
        "        \"pretrained_settings\": pretrained_settings[\"resnet50\"],\n",
        "        \"params\": {\n",
        "            \"out_channels\": (3, 64, 256, 512, 1024, 2048),\n",
        "            \"block\": Bottleneck,\n",
        "            \"layers\": [3, 4, 6, 3],\n",
        "        },\n",
        "    },\n",
        "    #\"resnet101\": {\n",
        "        #\"encoder\": ResNetEncoder,\n",
        "        #\"pretrained_settings\": pretrained_settings[\"resnet101\"],\n",
        "        #\"params\": {\n",
        "            #\"out_channels\": (3, 64, 256, 512, 1024, 2048),\n",
        "            #\"block\": Bottleneck,\n",
        "            #\"layers\": [3, 4, 23, 3],\n",
        "        #},\n",
        "    #},\n",
        "    #\"resnet152\": {\n",
        "        #\"encoder\": ResNetEncoder,\n",
        "        #\"pretrained_settings\": pretrained_settings[\"resnet152\"],\n",
        "        #\"params\": {\n",
        "            #\"out_channels\": (3, 64, 256, 512, 1024, 2048),\n",
        "            #\"block\": Bottleneck,\n",
        "            #\"layers\": [3, 8, 36, 3],\n",
        "        #},\n",
        "    #},\n",
        "    \"resnext50_32x4d\": {\n",
        "        \"encoder\": ResNetEncoder,\n",
        "        \"pretrained_settings\": pretrained_settings[\"resnext50_32x4d\"],\n",
        "        \"params\": {\n",
        "            \"out_channels\": (3, 64, 256, 512, 1024, 2048),\n",
        "            \"block\": Bottleneck,\n",
        "            \"layers\": [3, 4, 6, 3],\n",
        "            \"groups\": 32,\n",
        "            \"width_per_group\": 4,\n",
        "        },\n",
        "    },\n",
        "    \"resnext101_32x4d\": {\n",
        "        \"encoder\": ResNetEncoder,\n",
        "        \"pretrained_settings\": pretrained_settings[\"resnext101_32x4d\"],\n",
        "        \"params\": {\n",
        "            \"out_channels\": (3, 64, 256, 512, 1024, 2048),\n",
        "            \"block\": Bottleneck,\n",
        "            \"layers\": [3, 4, 23, 3],\n",
        "            \"groups\": 32,\n",
        "            \"width_per_group\": 4,\n",
        "        },\n",
        "    },\n",
        "    \"resnext101_32x8d\": {\n",
        "        \"encoder\": ResNetEncoder,\n",
        "        \"pretrained_settings\": pretrained_settings[\"resnext101_32x8d\"],\n",
        "        \"params\": {\n",
        "            \"out_channels\": (3, 64, 256, 512, 1024, 2048),\n",
        "            \"block\": Bottleneck,\n",
        "            \"layers\": [3, 4, 23, 3],\n",
        "            \"groups\": 32,\n",
        "            \"width_per_group\": 8,\n",
        "        },\n",
        "    },\n",
        "    \"resnext101_32x16d\": {\n",
        "        \"encoder\": ResNetEncoder,\n",
        "        \"pretrained_settings\": pretrained_settings[\"resnext101_32x16d\"],\n",
        "        \"params\": {\n",
        "            \"out_channels\": (3, 64, 256, 512, 1024, 2048),\n",
        "            \"block\": Bottleneck,\n",
        "            \"layers\": [3, 4, 23, 3],\n",
        "            \"groups\": 32,\n",
        "            \"width_per_group\": 16,\n",
        "        },\n",
        "    },\n",
        "    \"resnext101_32x32d\": {\n",
        "        \"encoder\": ResNetEncoder,\n",
        "        \"pretrained_settings\": pretrained_settings[\"resnext101_32x32d\"],\n",
        "        \"params\": {\n",
        "            \"out_channels\": (3, 64, 256, 512, 1024, 2048),\n",
        "            \"block\": Bottleneck,\n",
        "            \"layers\": [3, 4, 23, 3],\n",
        "            \"groups\": 32,\n",
        "            \"width_per_group\": 32,\n",
        "        },\n",
        "    },\n",
        "    \"resnext101_32x48d\": {\n",
        "        \"encoder\": ResNetEncoder,\n",
        "        \"pretrained_settings\": pretrained_settings[\"resnext101_32x48d\"],\n",
        "        \"params\": {\n",
        "            \"out_channels\": (3, 64, 256, 512, 1024, 2048),\n",
        "            \"block\": Bottleneck,\n",
        "            \"layers\": [3, 4, 23, 3],\n",
        "            \"groups\": 32,\n",
        "            \"width_per_group\": 48,\n",
        "        },\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VfQEJhvRnmD"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import time\n",
        "import functools\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "#from .resnet import resnet_encoders\n",
        "#from .utils import preprocess_input, TimmUniversalEncoder\n",
        "#from .timm_resnet import timm_resnest_encoders\n",
        "\n",
        "\n",
        "encoders = {}\n",
        "encoders.update(resnet_encoders)\n",
        "\n",
        "def get_encoder(name, in_channels=3, depth=5, weights=None, output_stride=32, **kwargs):\n",
        "\n",
        "    try:\n",
        "        Encoder = encoders[name][\"encoder\"]\n",
        "    except KeyError:\n",
        "        raise KeyError(\"Wrong encoder name `{}`, supported encoders: {}\".format(name, list(encoders.keys())))\n",
        "\n",
        "    params = encoders[name][\"params\"]\n",
        "    params.update(depth=depth)\n",
        "    encoder = Encoder(**params)\n",
        "\n",
        "    if weights is not None:\n",
        "        try:\n",
        "            settings = encoders[name][\"pretrained_settings\"][weights]\n",
        "\n",
        "        except KeyError:\n",
        "            raise KeyError(\n",
        "                \"Wrong pretrained weights `{}` for encoder `{}`. Available options are: {}\".format(\n",
        "                    weights,\n",
        "                    name,\n",
        "                    list(encoders[name][\"pretrained_settings\"].keys()),\n",
        "                )\n",
        "            )\n",
        "        encoder.load_state_dict(model_zoo.load_url(settings[\"url\"]))\n",
        "\n",
        "    encoder.set_in_channels(in_channels, pretrained=weights is not None)\n",
        "\n",
        "    if output_stride != 32:\n",
        "        encoder.make_dilated(output_stride)\n",
        "\n",
        "    return encoder\n",
        "\n",
        "\n",
        "def get_encoder_names():\n",
        "    return list(encoders.keys())\n",
        "\n",
        "\n",
        "def get_preprocessing_params(encoder_name, pretrained=\"imagenet\"):\n",
        "\n",
        "    all_settings = encoders[encoder_name][\"pretrained_settings\"]\n",
        "    if pretrained not in all_settings.keys():\n",
        "        raise ValueError(\"Available pretrained options {}\".format(all_settings.keys()))\n",
        "\n",
        "    settings = all_settings[pretrained]\n",
        "\n",
        "    formatted_settings = {}\n",
        "    formatted_settings[\"input_space\"] = settings.get(\"input_space\", \"RGB\")\n",
        "    formatted_settings[\"input_range\"] = list(settings.get(\"input_range\", [0, 1]))\n",
        "    formatted_settings[\"mean\"] = list(settings.get(\"mean\"))\n",
        "    formatted_settings[\"std\"] = list(settings.get(\"std\"))\n",
        "\n",
        "    return formatted_settings\n",
        "\n",
        "\n",
        "def get_preprocessing_fn(encoder_name, pretrained=\"imagenet\"):\n",
        "    params = get_preprocessing_params(encoder_name, pretrained=pretrained)\n",
        "    return functools.partial(preprocess_input, **params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#image augmentation\n",
        "import torchvision.transforms as transforms\n",
        "mean=0.5\n",
        "std_deviation=0.5\n",
        "\n",
        "CHANNELS_IMG=3\n",
        "transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.Resize(256),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([mean for _ in range(CHANNELS_IMG)],[std_deviation for _ in range(CHANNELS_IMG)]),\n",
        "\n",
        "     ]\n",
        ")"
      ],
      "metadata": {
        "id": "ZiwobdX_XF01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwlx1_JXV4A4",
        "outputId": "891b1957-ffef-42ea-b7dc-be16419f015d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "433\n",
            "torch.Size([3, 640, 640]) torch.Size([1, 640, 640]) tensor(2.6400, dtype=torch.float64) tensor(1.) tensor(-2.1179, dtype=torch.float64) tensor(0.)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "\n",
        "import cv2\n",
        "import glob\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "\n",
        "\n",
        "class BrainTumorDataset(data.Dataset):\n",
        "    def __init__(self, datafolder='/content/gdrive/MyDrive/train_new', transforms = transforms, train=True,\n",
        "                        image_size=(640, 640), train_test_ratio=0.15, preprocessing_fun=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.train = train\n",
        "        self.transform = transforms\n",
        "        self.image_size = image_size\n",
        "        self.train_test_ratio = train_test_ratio\n",
        "        self.preprocessing_fun = preprocessing_fun\n",
        "\n",
        "        image_names = [ ]\n",
        "        image_names_jpg = glob.glob(os.path.join(datafolder, '*.jpg'))\n",
        "        image_names_jpeg = glob.glob(os.path.join(datafolder, '*.jpeg'))\n",
        "        image_names_png  = glob.glob(os.path.join(datafolder, '*.png'))\n",
        "        image_names.extend(image_names_jpg)\n",
        "        image_names.extend(image_names_jpeg)\n",
        "        image_names.extend(image_names_png)\n",
        "\n",
        "        Num_testSamples = int(train_test_ratio*len(image_names))\n",
        "        random.seed(42)\n",
        "        random.shuffle(image_names)\n",
        "\n",
        "        if self.train:\n",
        "            self.image_names = image_names[Num_testSamples:]\n",
        "        else:\n",
        "            self.image_names = image_names[:Num_testSamples]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def read_image(self, image_name):\n",
        "        img = cv2.imread(image_name, cv2.IMREAD_GRAYSCALE)\n",
        "        try:\n",
        "            Img = cv2.resize(img, self.image_size, cv2.INTER_CUBIC)\n",
        "            Img = 255.0*(Img/ Img.max())\n",
        "            return Img\n",
        "        except:\n",
        "            raise ValueError('Image not found')\n",
        "\n",
        "    def add_rice_noise(self, img, snr=10, mu=0.1, sigma=1):\n",
        "        level = snr* np.max(img) / 100\n",
        "        size = img.shape\n",
        "        x = level* np.random.normal(mu, sigma, size=size) + img\n",
        "        y = level * np.random.normal(mu, sigma, size=size)\n",
        "        return np.sqrt(x**2 + y**2).astype(np.int16)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        image_name = self.image_names[index]\n",
        "        Img = self.read_image(image_name)\n",
        "        Img_noisy = self.add_rice_noise(Img)\n",
        "\n",
        "        Img_noisy = np.stack([Img_noisy, Img_noisy, Img_noisy], axis=-1)\n",
        "        Img = np.expand_dims(Img, axis=-1)\n",
        "\n",
        "\n",
        "        Img = torch.from_numpy(Img).to(torch.float32)\n",
        "        Img_noisy = torch.from_numpy(Img_noisy).to(torch.float32)\n",
        "\n",
        "        Img = Img/ Img.max()\n",
        "        Img_noisy = (Img_noisy - Img_noisy.min()) / (Img_noisy.max() - Img_noisy.min())\n",
        "        if self.preprocessing_fun:\n",
        "            Img_noisy = self.preprocessing_fun(Img_noisy)\n",
        "\n",
        "        Img_noisy = Img_noisy.permute(2, 0, 1)\n",
        "        Img = Img.permute(2, 0, 1)\n",
        "        return Img_noisy, Img\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    #from encoder.encoder_model import get_preprocessing_fn\n",
        "    encoder_name = 'resnet18'\n",
        "    preprocessing =  get_preprocessing_fn(encoder_name=encoder_name, pretrained='ssl')\n",
        "    dataset = BrainTumorDataset(preprocessing_fun=preprocessing)\n",
        "    print(len(dataset))\n",
        "    Img_noisy, Img = dataset[0]\n",
        "    print(Img_noisy.shape, Img.shape, Img_noisy.max(), Img.max(), Img_noisy.min(), Img.min())\n",
        "\n",
        "    #cv2.imshow('noisy image', np.uint8(255*Img_noisy.numpy().transpose(1, 2, 0)[:,:, 0]))\n",
        "    #cv2.imshow('ground image', np.uint8(255*Img.numpy().transpose(1, 2, 0)))\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AyyY4R9WZCu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def initialize_decoder(module):\n",
        "    for m in module.modules():\n",
        "\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_uniform_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            nn.init.constant_(m.weight, 1)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "def initialize_head(module):\n",
        "    for m in module.modules():\n",
        "        if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JU7YU89OopW_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "try:\n",
        "    from inplace_abn import InPlaceABN\n",
        "except ImportError:\n",
        "    InPlaceABN = None\n",
        "\n",
        "class Conv2dReLU(nn.Sequential):\n",
        "    def __init__( self,   in_channels,   out_channels, kernel_size, padding=0,   stride=1,   use_batchnorm=True, ):\n",
        "        conv = nn.Conv2d(in_channels,   out_channels,  kernel_size,\n",
        "                  stride=stride,   padding=padding,  bias=not (use_batchnorm), )\n",
        "\n",
        "        relu = nn.ReLU(inplace=True)\n",
        "        if use_batchnorm:\n",
        "            bn = nn.BatchNorm2d(out_channels)\n",
        "        else:\n",
        "            bn = nn.Identity()\n",
        "        super(Conv2dReLU, self).__init__(conv, bn, relu)\n",
        "\n",
        "\n",
        "class SCSEModule(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super().__init__()\n",
        "        self.cSE = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(in_channels, in_channels//reduction, 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels//reduction, in_channels, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.sSE =  nn.Sequential(nn.Conv2d(in_channels, 1, 1), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.cSE(x) + x * self.sSE(x)\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__( self,  in_channels,  skip_channels,  out_channels,  use_batchnorm=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = Conv2dReLU( in_channels + skip_channels, out_channels, kernel_size=3, padding=1,\n",
        "                                 use_batchnorm=use_batchnorm, )\n",
        "\n",
        "        self.attention1 = SCSEModule(in_channels=in_channels + skip_channels)\n",
        "        self.conv2 = Conv2dReLU( out_channels, out_channels, kernel_size=3, padding=1, use_batchnorm=use_batchnorm,)\n",
        "        self.attention2 = SCSEModule( in_channels=out_channels)\n",
        "\n",
        "    def forward(self, x, skip=None):\n",
        "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
        "        if skip is not None:\n",
        "            x = torch.cat([x, skip], dim=1)\n",
        "            x = self.attention1(x)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.attention2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UnetDecoder(nn.Module):\n",
        "    def __init__( self, encoder_channels, decoder_channels, n_blocks=5, use_batchnorm=True,):\n",
        "        super().__init__()\n",
        "\n",
        "        if n_blocks != len(decoder_channels):\n",
        "            raise ValueError(\"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(\n",
        "                    n_blocks, len(decoder_channels)))\n",
        "\n",
        "        encoder_channels = encoder_channels[1:]\n",
        "        encoder_channels = encoder_channels[::-1]\n",
        "\n",
        "        head_channels = encoder_channels[0]\n",
        "        in_channels = [head_channels] + list(decoder_channels[:-1])\n",
        "        skip_channels = list(encoder_channels[1:]) + [0]\n",
        "        out_channels = decoder_channels\n",
        "\n",
        "        blocks = [ DecoderBlock(in_ch, skip_ch, out_ch, use_batchnorm=use_batchnorm)\n",
        "            for in_ch, skip_ch, out_ch in zip(in_channels, skip_channels, out_channels)\n",
        "        ]\n",
        "        self.blocks = nn.ModuleList(blocks)\n",
        "\n",
        "    def forward(self, *features):\n",
        "\n",
        "        features = features[1:]\n",
        "        features = features[::-1]\n",
        "        x = features[0]\n",
        "        skips = features[1:]\n",
        "\n",
        "        for i, decoder_block in enumerate(self.blocks):\n",
        "            skip = skips[i] if i < len(skips) else None\n",
        "            x = decoder_block(x, skip)\n",
        "        return x\n",
        "\n",
        "\n",
        "#if __name__ == '__main__':\n",
        "    #model = UnetDecoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLSLNzT6X-Mu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#from encoder.encoder_model import get_encoder\n",
        "#from modules import UnetDecoder\n",
        "#from initialization import initialize_decoder, initialize_head\n",
        "\n",
        "\n",
        "class OutputLayer(nn.Sequential):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, activation=None):\n",
        "        conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n",
        "        super().__init__(conv2d)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class UNeT(nn.Module):\n",
        "    def __init__(self, encoder_name: str = \"resnet18\",  encoder_depth: int = 5,in_channels: int = 3,\n",
        "                   decoder_channels: List[int] = (256, 128, 64, 32, 16),):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = get_encoder(encoder_name, in_channels=in_channels, depth=encoder_depth)\n",
        "        self.decoder = UnetDecoder(self.encoder.out_channels, decoder_channels=decoder_channels, n_blocks= encoder_depth)\n",
        "        self.output  = OutputLayer(decoder_channels[-1], out_channels=1)\n",
        "        self.iniitialize()\n",
        "\n",
        "    def check_input_shape(self, x):\n",
        "        h, w = x.shape[-2:]\n",
        "        output_stride = self.encoder.output_stride\n",
        "        if h % output_stride != 0 or w % output_stride != 0:\n",
        "            new_h = (h // output_stride + 1) * output_stride if h % output_stride != 0 else h\n",
        "            new_w = (w // output_stride + 1) * output_stride if w % output_stride != 0 else w\n",
        "            raise RuntimeError(f\"Wrong input shape height={h}, width={w}. Expected image height and width \"\n",
        "                f\"divisible by {output_stride}. Consider pad your images to shape ({new_h}, {new_w}).\")\n",
        "\n",
        "    def iniitialize(self):\n",
        "        initialize_decoder(self.decoder)\n",
        "        initialize_head(self.output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.check_input_shape(x)\n",
        "        features = self.encoder(x)\n",
        "        decoder_output = self.decoder(*features)\n",
        "        output = self.output(decoder_output)\n",
        "        return output\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model  =UNeT()\n",
        "    #print(model)\n",
        "\n",
        "    x = torch.randn(1, 3, 224, 224)\n",
        "    out = model(x)\n",
        "    #print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchview"
      ],
      "metadata": {
        "id": "23Tb91u8Qlgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torchview import draw_graph\n",
        "# import graphviz\n",
        "\n",
        "# # when running on VSCode run the below command\n",
        "# # svg format on vscode does not give desired result\n",
        "# graphviz.set_jupyter_format('png')\n",
        "# model_graph = draw_graph(model, input_size=(32,3,640,640), device='meta',expand_nested=False)\n",
        "# model_graph.visual_graph"
      ],
      "metadata": {
        "id": "UAam9CB8QVRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyQHuYjepAVc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torchvision\n",
        "\n",
        "class VGGPerceptualLoss(torch.nn.Module):\n",
        "    def __init__(self, resize=True):\n",
        "        super(VGGPerceptualLoss, self).__init__()\n",
        "        blocks = [ ]\n",
        "        blocks.append(torchvision.models.vgg16(pretrained=True).features[:4].eval())\n",
        "        blocks.append(torchvision.models.vgg16(pretrained=True).features[4:9].eval())\n",
        "        blocks.append(torchvision.models.vgg16(pretrained=True).features[9:16].eval())\n",
        "        blocks.append(torchvision.models.vgg16(pretrained=True).features[16:23].eval())\n",
        "\n",
        "        for bl in blocks:\n",
        "            for p in bl.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        self.blocks = torch.nn.ModuleList(blocks)\n",
        "        self.transform = torch.nn.functional.interpolate\n",
        "        self.resize = resize\n",
        "        self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
        "        self.register_buffer(\"std\", torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
        "\n",
        "    def forward(self, input_image, target, feature_layers=[0, 1, 2, 3], style_layers=[]):\n",
        "        if input_image.shape[1] != 3:\n",
        "            input_image = input_image.repeat(1, 3, 1, 1)\n",
        "            target = target.repeat(1, 3, 1, 1)\n",
        "\n",
        "        input_image = (input_image - self.mean) / self.std\n",
        "        target = (target - self.mean) / self.std\n",
        "        if self.resize:\n",
        "            input_image = self.transform(input_image, mode='bilinear', size=(224, 224), align_corners=False)\n",
        "            target = self.transform(target, mode='bilinear', size=(224, 224), align_corners=False)\n",
        "\n",
        "        loss = 0.0\n",
        "        x = input_image\n",
        "        y = target\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            x = block(x)\n",
        "            y = block(y)\n",
        "            if i in feature_layers:\n",
        "                loss += torch.nn.functional.l1_loss(x, y)\n",
        "\n",
        "            if i in style_layers:\n",
        "                act_x = x.reshape(x.shape[0], x.shape[1], -1)\n",
        "                act_y = y.reshape(y.shape[0], y.shape[1], -1)\n",
        "                gram_x = act_x @ act_x.permute(0, 2, 1)\n",
        "                gram_y = act_y @ act_y.permute(0, 2, 1)\n",
        "                loss += torch.nn.functional.l1_loss(gram_x, gram_y)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8N51goAgpEq0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def gaussian(window_size, sigma):\n",
        "    gauss = torch.Tensor(\n",
        "        [\n",
        "            math.exp(-((x - window_size // 2) ** 2) / float(2 * sigma ** 2))\n",
        "            for x in range(window_size)\n",
        "        ]\n",
        "    )\n",
        "    return gauss / gauss.sum()\n",
        "\n",
        "\n",
        "def create_window(window_size, channel=1):\n",
        "\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float()\n",
        "    _2D_window /= _2D_window.sum()\n",
        "    _2D_window = _2D_window.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    window = torch.Tensor(\n",
        "        _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
        "    )\n",
        "\n",
        "    return window\n",
        "\n",
        "\n",
        "def ssim(\n",
        "    img1, img2, val_range, window_size=11, window=None, size_average=True, full=False):\n",
        "\n",
        "    L = val_range  # L is the dynamic range of the pixel values (255 for 8-bit grayscale images),\n",
        "\n",
        "    pad = window_size // 2\n",
        "\n",
        "    try:\n",
        "        _, channels, height, width = img1.size()\n",
        "    except:\n",
        "        channels, height, width = img1.size()\n",
        "\n",
        "    # if window is not provided, init one\n",
        "    if window is None:\n",
        "        real_size = min(window_size, height, width)  # window should be atleast 11x11\n",
        "        window = create_window(real_size, channel=channels).to(img1.device)\n",
        "\n",
        "    # calculating the mu parameter (locally) for both images using a gaussian filter\n",
        "    # calculates the luminosity params\n",
        "    mu1 = F.conv2d(img1, window, padding=pad, groups=channels)\n",
        "    mu2 = F.conv2d(img2, window, padding=pad, groups=channels)\n",
        "\n",
        "    mu1_sq = mu1 ** 2\n",
        "    mu2_sq = mu2 ** 2\n",
        "    mu12 = mu1 * mu2\n",
        "\n",
        "    # now we calculate the sigma square parameter\n",
        "    # Sigma deals with the contrast component\n",
        "    sigma1_sq = F.conv2d(img1 * img1, window, padding=pad, groups=channels) - mu1_sq\n",
        "    sigma2_sq = F.conv2d(img2 * img2, window, padding=pad, groups=channels) - mu2_sq\n",
        "    sigma12 = F.conv2d(img1 * img2, window, padding=pad, groups=channels) - mu12\n",
        "\n",
        "    # Some constants for stability\n",
        "    C1 = (0.01) ** 2  # NOTE: Removed L from here (ref PT implementation)\n",
        "    C2 = (0.03) ** 2\n",
        "\n",
        "    contrast_metric = (2.0 * sigma12 + C2) / (sigma1_sq + sigma2_sq + C2)\n",
        "    contrast_metric = torch.mean(contrast_metric)\n",
        "\n",
        "    numerator1 = 2 * mu12 + C1\n",
        "    numerator2 = 2 * sigma12 + C2\n",
        "    denominator1 = mu1_sq + mu2_sq + C1\n",
        "    denominator2 = sigma1_sq + sigma2_sq + C2\n",
        "\n",
        "    ssim_score = (numerator1 * numerator2) / (denominator1 * denominator2)\n",
        "\n",
        "    if size_average:\n",
        "        ret = ssim_score.mean()\n",
        "    else:\n",
        "        ret = ssim_score.mean(1).mean(1).mean(1)\n",
        "\n",
        "    if full:\n",
        "        return ret, contrast_metric\n",
        "\n",
        "    return ret"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def calculate_psnr(image1, image2):\n",
        "\n",
        "    if image1.shape != image2.shape:\n",
        "        raise ValueError(\"Images must have the same dimensions\")\n",
        "\n",
        "\n",
        "    device1 = torch.device('cpu')\n",
        "    image1 = image1.to(device1)\n",
        "    image2 = image2.to(device1)\n",
        "    # Calculate (MSE)\n",
        "    mse = F.mse_loss(image1.float(), image2.float()).item()\n",
        "\n",
        "    # Calculate the maximum pixel value\n",
        "    max_pixel = torch.max(image1.float())\n",
        "\n",
        "    # Calculate the PSNR value\n",
        "    psnr = 10 * torch.log10((max_pixel ** 2) / mse)\n",
        "\n",
        "    return psnr\n",
        "\n"
      ],
      "metadata": {
        "id": "J3F7DaICXTGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QwZLasppI5L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "\n",
        "\n",
        "def save_model(checkpoint, checkpoint_path):\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, model_opt):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    model_opt.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    return model, model_opt, checkpoint['epoch']\n",
        "\n",
        "\n",
        "def save_figure(model, loader, epoch, device, mean = [0.485, 0.456, 0.406], std= [0.229, 0.224, 0.225]):\n",
        "    model.eval()\n",
        "\n",
        "    if not os.path.exists(os.path.join(os.getcwd(), 'save_results')):\n",
        "        os.mkdir(os.path.join(os.getcwd(), 'save_results'))\n",
        "\n",
        "    result_dir = os.path.join(os.getcwd(), 'save_results')\n",
        "    if not os.path.exists(os.path.join(result_dir, str(epoch))):\n",
        "        os.mkdir(os.path.join(result_dir, str(epoch)))\n",
        "\n",
        "    save_dir = os.path.join(result_dir, str(epoch))\n",
        "    for idx, (x, y) in enumerate(loader):\n",
        "        x = x.to(device).float()\n",
        "        with torch.no_grad():\n",
        "            out = torch.sigmoid(model(x))\n",
        "            torchvision.utils.save_image(out[2,:], f\"{save_dir}/pred_{idx}.png\")\n",
        "            torchvision.utils.save_image(y[2,:],  f\"{save_dir}/target_{idx}.png\")\n",
        "            x = x.permute(0, 2, 3, 1).cpu().numpy()\n",
        "            x = (x *np.array(std)) + np.array(mean)\n",
        "            x = x / x.max()\n",
        "            x = np.transpose(x, (0, 3, 1, 2))\n",
        "            x = torch.from_numpy(x).float().to(device)\n",
        "            torchvision.utils.save_image(x[2,:], f\"{save_dir}/pnoisy_{idx}.png\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD6tvrkXW3SB",
        "outputId": "fb9743e5-d3df-424b-f036-c60754b83cfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:13<00:00, 41.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "#from ssim import ssim\n",
        "#from PerceptualLoss import  VGGPerceptualLoss\n",
        "\n",
        "criterion = VGGPerceptualLoss()\n",
        "\n",
        "def train(model, loader, optimizer, device, ):\n",
        "\n",
        "    model.train()\n",
        "    training_loss = 0.0\n",
        "    avg_ssim = 0.0\n",
        "    criterion = VGGPerceptualLoss()\n",
        "    avg_psnr=0.0\n",
        "\n",
        "    for i ,(x, y) in tqdm(enumerate(loader), total=len(loader), leave=False):\n",
        "\n",
        "        x = x.to(device).float()\n",
        "        y = y.to(device).float()\n",
        "        criterion = criterion.to(device)\n",
        "\n",
        "        out = torch.sigmoid(model(x))\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # metrics = ssim(out, y, 1)\n",
        "        # metrics2=calculate_psnr(out,y)\n",
        "        loss = criterion(out, y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        training_loss += loss.item()\n",
        "        # avg_ssim += metrics.item()\n",
        "        # avg_psnr+=metrics2\n",
        "\n",
        "    training_loss = training_loss / len(loader)\n",
        "    # avg_ssim      = avg_ssim / len(loader)\n",
        "    # avg_psnr = avg_psnr/len(loader)\n",
        "    return model, training_loss, avg_ssim,avg_psnr\n",
        "\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    avg_ssim = 0.0\n",
        "    avg_psnr=0.0\n",
        "    criterion = VGGPerceptualLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i ,(x, y) in tqdm(enumerate(loader), total=len(loader), leave=False):\n",
        "\n",
        "            x = x.to(device).float()\n",
        "            y = y.to(device).float()\n",
        "            criterion = criterion.to(device)\n",
        "\n",
        "            out = model(x)\n",
        "            out = torch.sigmoid(out)\n",
        "\n",
        "            metrics = ssim(out, y, 1)\n",
        "            metrics2=calculate_psnr(out,y)\n",
        "            loss = criterion(out, y)\n",
        "\n",
        "\n",
        "            val_loss      += loss.item()\n",
        "            avg_ssim      += metrics.item()\n",
        "            avg_psnr+=metrics2\n",
        "\n",
        "        val_loss = val_loss / len(loader)\n",
        "        avg_ssim = avg_ssim / len(loader)\n",
        "        avg_psnr = avg_psnr/len(loader)\n",
        "        return model, val_loss, avg_ssim,avg_psnr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MoRHwUf7Wbgx",
        "outputId": "54124cc9-3a5d-467c-dcfa-e6c6dcdceb3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "433 76\n",
            "55 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 Training loss 1.9164997816085816 training ssim 0.0 training psnr 0.0                   validation loss 2.671071743965149 and testing ssim 0.19175611436367035 testing_psnr 3.361661434173584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 Training loss 1.6002556584098122 training ssim 0.0 training psnr 0.0                   validation loss 1.5102197170257567 and testing ssim 0.43548665642738343 testing_psnr 12.531476020812988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2 Training loss 1.4498348062688655 training ssim 0.0 training psnr 0.0                   validation loss 1.5234755158424378 and testing ssim 0.6815564930438995 testing_psnr 19.201953887939453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3 Training loss 1.3296704010529952 training ssim 0.0 training psnr 0.0                   validation loss 1.5401189804077149 and testing ssim 0.7015475392341614 testing_psnr 12.30384635925293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4 Training loss 1.2126917080445723 training ssim 0.0 training psnr 0.0                   validation loss 1.18134286403656 and testing ssim 0.8429835736751556 testing_psnr 30.337627410888672\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5 Training loss 1.1758254701440984 training ssim 0.0 training psnr 0.0                   validation loss 1.1852007746696471 and testing ssim 0.8343759417533875 testing_psnr 29.739639282226562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6 Training loss 1.1589943734082309 training ssim 0.0 training psnr 0.0                   validation loss 1.1722100496292114 and testing ssim 0.8393115222454071 testing_psnr 28.5677433013916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7 Training loss 1.146575893055309 training ssim 0.0 training psnr 0.0                   validation loss 1.1331689476966857 and testing ssim 0.8599086582660675 testing_psnr 27.827251434326172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 8 Training loss 1.1356224016709762 training ssim 0.0 training psnr 0.0                   validation loss 1.1333695650100708 and testing ssim 0.849591052532196 testing_psnr 28.81329345703125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 9 Training loss 1.128442627733404 training ssim 0.0 training psnr 0.0                   validation loss 1.1556755900382996 and testing ssim 0.853864973783493 testing_psnr 22.41476058959961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 10 Training loss 1.13030452078039 training ssim 0.0 training psnr 0.0                   validation loss 1.111140251159668 and testing ssim 0.8750343382358551 testing_psnr 30.442800521850586\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 11 Training loss 1.1268538193269209 training ssim 0.0 training psnr 0.0                   validation loss 1.1399505853652954 and testing ssim 0.869911390542984 testing_psnr 24.447711944580078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 12 Training loss 1.1090115677226673 training ssim 0.0 training psnr 0.0                   validation loss 1.1099905252456665 and testing ssim 0.8803780496120452 testing_psnr 29.672115325927734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 13 Training loss 1.1055316491560503 training ssim 0.0 training psnr 0.0                   validation loss 1.1365548372268677 and testing ssim 0.8695714473724365 testing_psnr 24.084339141845703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 14 Training loss 1.1010761239311913 training ssim 0.0 training psnr 0.0                   validation loss 1.1339321374893188 and testing ssim 0.8386745452880859 testing_psnr 31.67820167541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 15 Training loss 1.0933701385151255 training ssim 0.0 training psnr 0.0                   validation loss 1.1019372582435607 and testing ssim 0.8787555038928986 testing_psnr 31.798797607421875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 16 Training loss 1.0924328457225452 training ssim 0.0 training psnr 0.0                   validation loss 1.0866251349449159 and testing ssim 0.8909843683242797 testing_psnr 30.767629623413086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 17 Training loss 1.085430872440338 training ssim 0.0 training psnr 0.0                   validation loss 1.0791723489761353 and testing ssim 0.8909716308116913 testing_psnr 31.240489959716797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 18 Training loss 1.076842561635104 training ssim 0.0 training psnr 0.0                   validation loss 1.0892799019813537 and testing ssim 0.8963308990001678 testing_psnr 32.54130554199219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 19 Training loss 1.0675961125980724 training ssim 0.0 training psnr 0.0                   validation loss 1.0757130980491638 and testing ssim 0.8919422626495361 testing_psnr 29.649362564086914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 20 Training loss 1.071490770036524 training ssim 0.0 training psnr 0.0                   validation loss 1.0640599846839904 and testing ssim 0.9004649639129638 testing_psnr 31.687158584594727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 21 Training loss 1.0674367536198008 training ssim 0.0 training psnr 0.0                   validation loss 1.0789148211479187 and testing ssim 0.8778611660003662 testing_psnr 32.173362731933594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 22 Training loss 1.0561635819348423 training ssim 0.0 training psnr 0.0                   validation loss 1.0739826917648316 and testing ssim 0.896808260679245 testing_psnr 32.945621490478516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 23 Training loss 1.0691005966880105 training ssim 0.0 training psnr 0.0                   validation loss 1.058215892314911 and testing ssim 0.9020622491836547 testing_psnr 32.063148498535156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 24 Training loss 1.0607420964674517 training ssim 0.0 training psnr 0.0                   validation loss 1.0668768525123595 and testing ssim 0.8970088183879852 testing_psnr 30.872142791748047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 25 Training loss 1.0544050444256177 training ssim 0.0 training psnr 0.0                   validation loss 1.0665918111801147 and testing ssim 0.9036054134368896 testing_psnr 31.217121124267578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 26 Training loss 1.0505900101228194 training ssim 0.0 training psnr 0.0                   validation loss 1.0622804045677186 and testing ssim 0.8978386282920837 testing_psnr 31.54034996032715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 27 Training loss 1.0449575445868753 training ssim 0.0 training psnr 0.0                   validation loss 1.0458007097244262 and testing ssim 0.9040452659130096 testing_psnr 32.80711364746094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 28 Training loss 1.0431925827806645 training ssim 0.0 training psnr 0.0                   validation loss 1.0657117009162902 and testing ssim 0.9003380358219146 testing_psnr 30.715002059936523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 29 Training loss 1.0395240577784453 training ssim 0.0 training psnr 0.0                   validation loss 1.0400505304336547 and testing ssim 0.9073967039585114 testing_psnr 31.47628402709961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 30 Training loss 1.0318170244043523 training ssim 0.0 training psnr 0.0                   validation loss 1.0330987930297852 and testing ssim 0.9097204685211182 testing_psnr 31.53072738647461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 31 Training loss 1.0354914806105875 training ssim 0.0 training psnr 0.0                   validation loss 1.0679779767990112 and testing ssim 0.889357328414917 testing_psnr 26.65981101989746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 32 Training loss 1.0375078168782321 training ssim 0.0 training psnr 0.0                   validation loss 1.0499867439270019 and testing ssim 0.8995946109294891 testing_psnr 27.339855194091797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 33 Training loss 1.0241117910905317 training ssim 0.0 training psnr 0.0                   validation loss 1.2366409778594971 and testing ssim 0.8478054761886596 testing_psnr 31.283077239990234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 34 Training loss 1.099770723689686 training ssim 0.0 training psnr 0.0                   validation loss 1.1188157796859741 and testing ssim 0.8398180186748505 testing_psnr 32.603515625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 35 Training loss 1.0614517244425687 training ssim 0.0 training psnr 0.0                   validation loss 1.0731002330780028 and testing ssim 0.8908677756786346 testing_psnr 28.672489166259766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 36 Training loss 1.047208349271254 training ssim 0.0 training psnr 0.0                   validation loss 1.0485719978809356 and testing ssim 0.9007649838924408 testing_psnr 32.521629333496094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 37 Training loss 1.0365160497752104 training ssim 0.0 training psnr 0.0                   validation loss 1.0461239695549012 and testing ssim 0.8963366329669953 testing_psnr 30.329044342041016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 38 Training loss 1.028973452611403 training ssim 0.0 training psnr 0.0                   validation loss 1.033259344100952 and testing ssim 0.9080093979835511 testing_psnr 31.443603515625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 39 Training loss 1.0292612314224243 training ssim 0.0 training psnr 0.0                   validation loss 1.0275913596153259 and testing ssim 0.9055325210094451 testing_psnr 31.31902503967285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 40 Training loss 1.028739699450406 training ssim 0.0 training psnr 0.0                   validation loss 1.0403760552406311 and testing ssim 0.8974111974239349 testing_psnr 28.391132354736328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 41 Training loss 1.0195616017688405 training ssim 0.0 training psnr 0.0                   validation loss 1.030051863193512 and testing ssim 0.8975040078163147 testing_psnr 31.74867820739746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 42 Training loss 1.0186860929835926 training ssim 0.0 training psnr 0.0                   validation loss 1.0210299253463746 and testing ssim 0.9102904319763183 testing_psnr 31.60199546813965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 43 Training loss 1.023649194023826 training ssim 0.0 training psnr 0.0                   validation loss 1.0559925436973572 and testing ssim 0.8714142739772797 testing_psnr 31.675174713134766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 44 Training loss 1.0243713931603866 training ssim 0.0 training psnr 0.0                   validation loss 1.2341767430305481 and testing ssim 0.8162465453147888 testing_psnr 31.895727157592773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 45 Training loss 1.0113177798011086 training ssim 0.0 training psnr 0.0                   validation loss 1.0128497838974 and testing ssim 0.9102264523506165 testing_psnr 32.03489303588867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 46 Training loss 1.0157788959416476 training ssim 0.0 training psnr 0.0                   validation loss 1.0255270183086396 and testing ssim 0.9077555000782013 testing_psnr 33.58214569091797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 47 Training loss 1.0065326755697077 training ssim 0.0 training psnr 0.0                   validation loss 1.0161556005477905 and testing ssim 0.912617951631546 testing_psnr 31.560022354125977\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 48 Training loss 1.0091414538296786 training ssim 0.0 training psnr 0.0                   validation loss 1.0143738925457 and testing ssim 0.9099540472030639 testing_psnr 32.087120056152344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 49 Training loss 1.0083039771426807 training ssim 0.0 training psnr 0.0                   validation loss 1.0221803665161133 and testing ssim 0.90605388879776 testing_psnr 30.574615478515625\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAHWCAYAAACbsXOkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB+/klEQVR4nO3dd3hUVf7H8ffMJJn0SioJhF4lIE1QFAVEVBRQV/3hCnZd0GVRd8WCiK646q69rgVcxS7YUUBAUVBaqIKAoaXQk0kvM/f3xyQDYwKEZGBmwuf1PPeZmTvn3jkT77L55JzzvSbDMAxERERERESkUcze7oCIiIiIiEhToHAlIiIiIiLiAQpXIiIiIiIiHqBwJSIiIiIi4gEKVyIiIiIiIh6gcCUiIiIiIuIBClciIiIiIiIeoHAlIiIiIiLiAQpXIiIiIiIiHqBwJSLip8aOHUt6enqDjp0yZQomk8mzHfIx27Ztw2QyMX369JP+2SaTiSlTprheT58+HZPJxLZt2455bHp6OmPHjvVofxpzrYiISP0pXImIeJjJZKrXtnDhQm939ZR3xx13YDKZ2LJlyxHb3HfffZhMJtasWXMSe3b8cnJymDJlCpmZmd7uiktNwH3yySe93RURkZMiwNsdEBFpav73v/+5vX7rrbeYO3durf2dOnVq1Of897//xeFwNOjY+++/n3vuuadRn98UjB49mueee46ZM2cyefLkOtu8++67nHbaaXTr1q3Bn/PnP/+Zq666CqvV2uBzHEtOTg4PPfQQ6enpdO/e3e29xlwrIiJSfwpXIiIeds0117i9Xrp0KXPnzq21/49KSkoIDQ2t9+cEBgY2qH8AAQEBBATo/wL69u1L27Zteffdd+sMV0uWLCErK4vHHnusUZ9jsViwWCyNOkdjNOZaERGR+tO0QBERLxg4cCBdu3ZlxYoVnH322YSGhnLvvfcC8Omnn3LRRReRkpKC1WqlTZs2PPzww9jtdrdz/HEdzeFTsF599VXatGmD1Wqld+/eLFu2zO3YutZcmUwmxo8fz+zZs+natStWq5UuXbowZ86cWv1fuHAhvXr1Ijg4mDZt2vDKK6/Uex3XDz/8wBVXXEGLFi2wWq2kpaXxt7/9jdLS0lrfLzw8nOzsbEaMGEF4eDjx8fHcddddtX4W+fn5jB07lqioKKKjoxkzZgz5+fnH7As4R682btzIypUra703c+ZMTCYTV199NRUVFUyePJmePXsSFRVFWFgYAwYMYMGCBcf8jLrWXBmGwSOPPEJqaiqhoaGce+65rF+/vtaxBw4c4K677uK0004jPDycyMhIhg0bxurVq11tFi5cSO/evQG47rrrXFNPa9ab1bXmqri4mDvvvJO0tDSsVisdOnTgySefxDAMt3bHc1001J49e7jhhhtITEwkODiYjIwMZsyYUavde++9R8+ePYmIiCAyMpLTTjuNZ555xvV+ZWUlDz30EO3atSM4OJi4uDjOOuss5s6d67G+iogcjf5sKSLiJfv372fYsGFcddVVXHPNNSQmJgLOX8TDw8OZOHEi4eHhfPfdd0yePBmbzcYTTzxxzPPOnDmTwsJCbrnlFkwmE48//jijRo3i999/P+YIxuLFi/nkk0/4y1/+QkREBM8++yyXXXYZO3bsIC4uDoBVq1ZxwQUXkJyczEMPPYTdbmfq1KnEx8fX63t/+OGHlJSUcNtttxEXF8cvv/zCc889x65du/jwww/d2trtdoYOHUrfvn158sknmTdvHv/+979p06YNt912G+AMKZdeeimLFy/m1ltvpVOnTsyaNYsxY8bUqz+jR4/moYceYubMmZx++ulun/3BBx8wYMAAWrRowb59+3jttde4+uqruemmmygsLOT1119n6NCh/PLLL7Wm4h3L5MmTeeSRR7jwwgu58MILWblyJeeffz4VFRVu7X7//Xdmz57NFVdcQatWrdi9ezevvPIK55xzDhs2bCAlJYVOnToxdepUJk+ezM0338yAAQMA6N+/f52fbRgGl1xyCQsWLOCGG26ge/fufPPNN9x9991kZ2fz1FNPubWvz3XRUKWlpQwcOJAtW7Ywfvx4WrVqxYcffsjYsWPJz8/nr3/9KwBz587l6quvZtCgQfzrX/8C4Ndff+XHH390tZkyZQrTpk3jxhtvpE+fPthsNpYvX87KlSsZMmRIo/opIlIvhoiInFDjxo0z/vjP7TnnnGMAxssvv1yrfUlJSa19t9xyixEaGmqUlZW59o0ZM8Zo2bKl63VWVpYBGHFxccaBAwdc+z/99FMDMD7//HPXvgcffLBWnwAjKCjI2LJli2vf6tWrDcB47rnnXPuGDx9uhIaGGtnZ2a59mzdvNgICAmqdsy51fb9p06YZJpPJ2L59u9v3A4ypU6e6te3Ro4fRs2dP1+vZs2cbgPH444+79lVVVRkDBgwwAOPNN988Zp969+5tpKamGna73bVvzpw5BmC88sorrnOWl5e7HXfw4EEjMTHRuP766932A8aDDz7oev3mm28agJGVlWUYhmHs2bPHCAoKMi666CLD4XC42t17770GYIwZM8a1r6yszK1fhuH8b221Wt1+NsuWLTvi9/3jtVLzM3vkkUfc2l1++eWGyWRyuwbqe13UpeaafOKJJ47Y5umnnzYA4+2333btq6ioMPr162eEh4cbNpvNMAzD+Otf/2pERkYaVVVVRzxXRkaGcdFFFx21TyIiJ5KmBYqIeInVauW6666rtT8kJMT1vLCwkH379jFgwABKSkrYuHHjMc975ZVXEhMT43pdM4rx+++/H/PYwYMH06ZNG9frbt26ERkZ6TrWbrczb948RowYQUpKiqtd27ZtGTZs2DHPD+7fr7i4mH379tG/f38Mw2DVqlW12t96661urwcMGOD2Xb766isCAgJcI1ngXON0++2316s/4Fwnt2vXLr7//nvXvpkzZxIUFMQVV1zhOmdQUBAADoeDAwcOUFVVRa9eveqcUng08+bNo6Kigttvv91tKuWECRNqtbVarZjNzv+7ttvt7N+/n/DwcDp06HDcn1vjq6++wmKxcMcdd7jtv/POOzEMg6+//tpt/7Gui8b46quvSEpK4uqrr3btCwwM5I477qCoqIhFixYBEB0dTXFx8VGn+EVHR7N+/Xo2b97c6H6JiDSEwpWIiJc0b97c9cv64davX8/IkSOJiooiMjKS+Ph4VzGMgoKCY563RYsWbq9rgtbBgweP+9ia42uO3bNnD6WlpbRt27ZWu7r21WXHjh2MHTuW2NhY1zqqc845B6j9/YKDg2tNNzy8PwDbt28nOTmZ8PBwt3YdOnSoV38ArrrqKiwWCzNnzgSgrKyMWbNmMWzYMLegOmPGDLp16+ZazxMfH8+XX35Zr/8uh9u+fTsA7dq1c9sfHx/v9nngDHJPPfUU7dq1w2q10qxZM+Lj41mzZs1xf+7hn5+SkkJERITb/poKljX9q3Gs66Ixtm/fTrt27VwB8kh9+ctf/kL79u0ZNmwYqampXH/99bXWfU2dOpX8/Hzat2/Paaedxt133+3zJfRFpGlRuBIR8ZLDR3Bq5Ofnc84557B69WqmTp3K559/zty5c11rTOpTTvtIVemMPxQq8PSx9WG32xkyZAhffvkl//jHP5g9ezZz5851FV744/c7WRX2EhISGDJkCB9//DGVlZV8/vnnFBYWMnr0aFebt99+m7Fjx9KmTRtef/115syZw9y5cznvvPNOaJnzRx99lIkTJ3L22Wfz9ttv88033zB37ly6dOly0sqrn+jroj4SEhLIzMzks88+c60XGzZsmNvaurPPPputW7fyxhtv0LVrV1577TVOP/10XnvttZPWTxE5tamghYiID1m4cCH79+/nk08+4eyzz3btz8rK8mKvDklISCA4OLjOm+4e7Ua8NdauXctvv/3GjBkzuPbaa137G1PNrWXLlsyfP5+ioiK30atNmzYd13lGjx7NnDlz+Prrr5k5cyaRkZEMHz7c9f5HH31E69at+eSTT9ym8j344IMN6jPA5s2bad26tWv/3r17a40GffTRR5x77rm8/vrrbvvz8/Np1qyZ63V9KjUe/vnz5s2jsLDQbfSqZtppTf9OhpYtW7JmzRocDofb6FVdfQkKCmL48OEMHz4ch8PBX/7yF1555RUeeOAB18hpbGws1113Hddddx1FRUWcffbZTJkyhRtvvPGkfScROXVp5EpExIfUjBAcPiJQUVHBiy++6K0uubFYLAwePJjZs2eTk5Pj2r9ly5Za63SOdDy4fz/DMNzKaR+vCy+8kKqqKl566SXXPrvdznPPPXdc5xkxYgShoaG8+OKLfP3114waNYrg4OCj9v3nn39myZIlx93nwYMHExgYyHPPPed2vqeffrpWW4vFUmuE6MMPPyQ7O9ttX1hYGEC9StBfeOGF2O12nn/+ebf9Tz31FCaTqd7r5zzhwgsvJC8vj/fff9+1r6qqiueee47w8HDXlNH9+/e7HWc2m103di4vL6+zTXh4OG3btnW9LyJyomnkSkTEh/Tv35+YmBjGjBnDHXfcgclk4n//+99JnX51LFOmTOHbb7/lzDPP5LbbbnP9kt61a1cyMzOPemzHjh1p06YNd911F9nZ2URGRvLxxx83au3O8OHDOfPMM7nnnnvYtm0bnTt35pNPPjnu9Ujh4eGMGDHCte7q8CmBABdffDGffPIJI0eO5KKLLiIrK4uXX36Zzp07U1RUdFyfVXO/rmnTpnHxxRdz4YUXsmrVKr7++mu30aiaz506dSrXXXcd/fv3Z+3atbzzzjtuI14Abdq0ITo6mpdffpmIiAjCwsLo27cvrVq1qvX5w4cP59xzz+W+++5j27ZtZGRk8O233/Lpp58yYcIEt+IVnjB//nzKyspq7R8xYgQ333wzr7zyCmPHjmXFihWkp6fz0Ucf8eOPP/L000+7RtZuvPFGDhw4wHnnnUdqairbt2/nueeeo3v37q71WZ07d2bgwIH07NmT2NhYli9fzkcffcT48eM9+n1ERI5E4UpExIfExcXxxRdfcOedd3L//fcTExPDNddcw6BBgxg6dKi3uwdAz549+frrr7nrrrt44IEHSEtLY+rUqfz666/HrGYYGBjI559/zh133MG0adMIDg5m5MiRjB8/noyMjAb1x2w289lnnzFhwgTefvttTCYTl1xyCf/+97/p0aPHcZ1r9OjRzJw5k+TkZM477zy398aOHUteXh6vvPIK33zzDZ07d+btt9/mww8/ZOHChcfd70ceeYTg4GBefvllFixYQN++ffn222+56KKL3Nrde++9FBcXM3PmTN5//31OP/10vvzyS+655x63doGBgcyYMYNJkyZx6623UlVVxZtvvllnuKr5mU2ePJn333+fN998k/T0dJ544gnuvPPO4/4uxzJnzpw6bzqcnp5O165dWbhwIffccw8zZszAZrPRoUMH3nzzTcaOHetqe8011/Dqq6/y4osvkp+fT1JSEldeeSVTpkxxTSe84447+Oyzz/j2228pLy+nZcuWPPLII9x9990e/04iInUxGb7051AREfFbI0aMUBlsERE5pWnNlYiIHLfS0lK315s3b+arr75i4MCB3umQiIiID9DIlYiIHLfk5GTGjh1L69at2b59Oy+99BLl5eWsWrWq1r2bREREThVacyUiIsftggsu4N133yUvLw+r1Uq/fv149NFHFaxEROSUppErERERERERD/Dqmqtp06bRu3dvIiIiSEhIYMSIEce86ePAgQMxmUy1tsOrK40dO7bW+xdccMGJ/joiIiIiInIK8+q0wEWLFjFu3Dh69+5NVVUV9957L+effz4bNmxw3Qzxjz755BMqKipcr/fv309GRgZXXHGFW7sLLriAN9980/XaarWemC8hIiIiIiKCl8PVH+95MX36dBISElixYgVnn312ncfExsa6vX7vvfcIDQ2tFa6sVitJSUkN6pfD4SAnJ4eIiAhMJlODziEiIiIiIv7PMAwKCwtJSUlx3VfvSHyqoEVBQQFQO0Adzeuvv85VV11Va6Rr4cKFJCQkEBMTw3nnnccjjzxCXFxcnecoLy+nvLzc9To7O5vOnTs34BuIiIiIiEhTtHPnTlJTU4/axmcKWjgcDi655BLy8/NZvHhxvY755Zdf6Nu3Lz///DN9+vRx7a8ZzWrVqhVbt27l3nvvJTw8nCVLlmCxWGqdZ8qUKTz00EO19u/cuZPIyMiGfykREREREfFrNpuNtLQ08vPziYqKOmpbnwlXt912G19//TWLFy8+ZiKsccstt7BkyRLWrFlz1Ha///47bdq0Yd68eQwaNKjW+38cuar5ARYUFChciYiIiIicwmw2G1FRUfXKBl6tFlhj/PjxfPHFFyxYsKDewaq4uJj33nuPG2644ZhtW7duTbNmzdiyZUud71utViIjI902ERERERGR4+HVNVeGYXD77bcza9YsFi5cSKtWrep97Icffkh5eTnXXHPNMdvu2rWL/fv3k5yc3JjuioiIiIiIHJFXR67GjRvH22+/zcyZM4mIiCAvL4+8vDxKS0tdba699lomTZpU69jXX3+dESNG1CpSUVRUxN13383SpUvZtm0b8+fP59JLL6Vt27YMHTr0hH8nERERERE5NXl15Oqll14CnDcGPtybb77J2LFjAdixY0etkoebNm1i8eLFfPvtt7XOabFYWLNmDTNmzCA/P5+UlBTOP/98Hn74Yd3rSkRERESaJLvdTmVlpbe74ZcsFgsBAQEeuQWTzxS08CXHs2hNRERERMSbioqK2LVrF/q1vuFCQ0NJTk4mKCio1nvHkw186j5XIiIiIiJSf3a7nV27dhEaGkp8fLxHRl9OJYZhUFFRwd69e8nKyqJdu3bHvFHw0ShciYiIiIj4qcrKSgzDID4+npCQEG93xy+FhIQQGBjI9u3bqaioIDg4uMHn8olS7CIiIiIi0nAasWqcxoxWuZ3HI2cRERERERE5xSlciYiIiIiIeIDClYiIiIiI+JX09HSefvppb3ejFoUrERERERERD1C4EhERERER8QCFKx93x7urGPTvhSzfdsDbXRERERERH2cYBiUVVV7Z6nsT41dffZWUlBQcDofb/ksvvZTrr7+erVu3cumll5KYmEh4eDi9e/dm3rx5J+LH5XG6z5WP23mwhK17i9lXVOHtroiIiIiIjyuttNN58jde+ewNU4cSGnTseHHFFVdw++23s2DBAgYNGgTAgQMHmDNnDl999RVFRUVceOGF/POf/8RqtfLWW28xfPhwNm3aRIsWLU7012gUjVz5uIjgQAAKyyq93BMRERERkcaLiYlh2LBhzJw507Xvo48+olmzZpx77rlkZGRwyy230LVrV9q1a8fDDz9MmzZt+Oyzz7zY6/rRyJWPiwh2/icqLKvyck9ERERExNeFBFrYMHWo1z67vkaPHs1NN93Eiy++iNVq5Z133uGqq67CbDZTVFTElClT+PLLL8nNzaWqqorS0lJ27NhxAnvvGQpXPi5S4UpERERE6slkMtVrap63DR8+HMMw+PLLL+nduzc//PADTz31FAB33XUXc+fO5cknn6Rt27aEhIRw+eWXU1Hh+8tkfP8nf4rTtEARERERaWqCg4MZNWoU77zzDlu2bKFDhw6cfvrpAPz444+MHTuWkSNHAlBUVMS2bdu82Nv6U7jycRFWjVyJiIiISNMzevRoLr74YtavX88111zj2t+uXTs++eQThg8fjslk4oEHHqhVWdBXqaCFj4sMqR65KtfIlYiIiIg0Heeddx6xsbFs2rSJ//u//3Pt/89//kNMTAz9+/dn+PDhDB061DWq5es0cuXjVNBCRERERJois9lMTk5Orf3p6el89913bvvGjRvn9tpXpwlq5MrH1ay5silciYiIiIj4NIUrH3do5ErTAkVEREREfJnClY+rCVe2Uo1ciYiIiIj4MoUrHxepUuwiIiIiIn5B4crH1YxclVc5qKjyjxKUIiIiIiKnIoUrHxduPVTQUaNXIiIiIiK+S+HKxwVYzIQGWQCVYxcRERER8WUKV35A97oSEREREfF9Cld+IEJFLUREREREfJ7ClR9wlWPXyJWIiIiIiJv09HSefvppb3cDgIBjNxFv08iViIiIiDQlAwcOpHv37h4JRcuWLSMsLKzxnfIAhSs/EKk1VyIiIiJyCjEMA7vdTkDAseNKfHz8SehR/WhaoB84NHKlcCUiIiIiR2EYUFHsnc0w6tXFsWPHsmjRIp555hlMJhMmk4np06djMpn4+uuv6dmzJ1arlcWLF7N161YuvfRSEhMTCQ8Pp3fv3sybN8/tfH+cFmgymXjttdcYOXIkoaGhtGvXjs8++8yTP+Uj0siVHzg0cqVpgSIiIiJyFJUl8GiKdz773hwIOvb0vGeeeYbffvuNrl27MnXqVADWr18PwD333MOTTz5J69atiYmJYefOnVx44YX885//xGq18tZbbzF8+HA2bdpEixYtjvgZDz30EI8//jhPPPEEzz33HKNHj2b79u3ExsZ65rsegUau/IBKsYuIiIhIUxEVFUVQUBChoaEkJSWRlJSExeK8r+vUqVMZMmQIbdq0ITY2loyMDG655Ra6du1Ku3btePjhh2nTps0xR6LGjh3L1VdfTdu2bXn00UcpKiril19+OeHfTSNXfqBmWqBNI1ciIiIicjSBoc4RJG99diP16tXL7XVRURFTpkzhyy+/JDc3l6qqKkpLS9mxY8dRz9OtWzfX87CwMCIjI9mzZ0+j+3csCld+QCNXIiIiIlIvJlO9pub5qj9W/bvrrruYO3cuTz75JG3btiUkJITLL7+cioqKo54nMDDQ7bXJZMLhcHi8v3+kcOUHVIpdRERERJqSoKAg7Hb7Mdv9+OOPjB07lpEjRwLOkaxt27ad4N41nNZc+QGNXImIiIhIU5Kens7PP//Mtm3b2Ldv3xFHldq1a8cnn3xCZmYmq1ev5v/+7/9OyghUQylc+YGacGVTuBIRERGRJuCuu+7CYrHQuXNn4uPjj7iG6j//+Q8xMTH079+f4cOHM3ToUE4//fST3Nv607RAPxCpaYEiIiIi0oS0b9+eJUuWuO0bO3ZsrXbp6el89913bvvGjRvn9vqP0wSNOu63lZ+f36B+Hi+NXPmBmpGr8ioHFVW+OwwqIiIiInIqU7jyA+HWQwOMGr0SEREREfFNCld+IMBiJizIeWM1FbUQEREREfFNXg1X06ZNo3fv3kRERJCQkMCIESPYtGnTUY+ZPn06JpPJbQsODnZrYxgGkydPJjk5mZCQEAYPHszmzZtP5Fc54Q6VY1e4EhERERHxRV4NV4sWLWLcuHEsXbqUuXPnUllZyfnnn09xcfFRj4uMjCQ3N9e1bd++3e39xx9/nGeffZaXX36Zn3/+mbCwMIYOHUpZWdmJ/Don1KFy7JoWKCIiIiLu6iriIPXnqZ+fV6sFzpkzx+319OnTSUhIYMWKFZx99tlHPM5kMpGUlFTne4Zh8PTTT3P//fdz6aWXAvDWW2+RmJjI7Nmzueqqqzz3BU4ilWMXERERkT+yWJxLRyoqKggJCfFyb/xXSUkJAIGBgY06j0+VYi8oKAAgNjb2qO2Kiopo2bIlDoeD008/nUcffZQuXboAkJWVRV5eHoMHD3a1j4qKom/fvixZsqTOcFVeXk55ebnrtc1m88TX8agIlWMXERERkT8ICAggNDSUvXv3EhgYiNmskgrHwzAMSkpK2LNnD9HR0a6w2lA+E64cDgcTJkzgzDPPpGvXrkds16FDB9544w26detGQUEBTz75JP3792f9+vWkpqaSl5cHQGJiottxiYmJrvf+aNq0aTz00EOe+zIngEauREREROSPTCYTycnJZGVl1VoqI/UXHR19xJlxx8NnwtW4ceNYt24dixcvPmq7fv360a9fP9fr/v3706lTJ1555RUefvjhBn32pEmTmDhxouu1zWYjLS2tQec6UTRyJSIiIiJ1CQoKol27dlRUVHi7K34pMDCw0SNWNXwiXI0fP54vvviC77//ntTU1OM6NjAwkB49erBlyxYAV+LcvXs3ycnJrna7d++me/fudZ7DarVitVob1vmTJNJV0EIjVyIiIiLizmw216qgLSefVydlGobB+PHjmTVrFt999x2tWrU67nPY7XbWrl3rClKtWrUiKSmJ+fPnu9rYbDZ+/vlntxEvf6NqgSIiIiIivs2rI1fjxo1j5syZfPrpp0RERLjWREVFRbmqnVx77bU0b96cadOmATB16lTOOOMM2rZtS35+Pk888QTbt2/nxhtvBJzzTidMmMAjjzxCu3btaNWqFQ888AApKSmMGDHCK9/TE3SfKxERERER3+bVcPXSSy8BMHDgQLf9b775JmPHjgVgx44dblVPDh48yE033UReXh4xMTH07NmTn376ic6dO7va/P3vf6e4uJibb76Z/Px8zjrrLObMmePXQ6URmhYoIiIiIuLTTIbuOFaLzWYjKiqKgoICIiMjvd0dAOZu2M1Nby0nIzWKT8ef5e3uiIiIiIicEo4nG6gQvp9QQQsREREREd+mcOUnatZc6T5XIiIiIiK+SeHKT6haoIiIiIiIb1O48hOR1SNX5VUOKqocXu6NiIiIiIj8kcKVnwgPPlTYUaNXIiIiIiK+R+HKT1jMJsKCLICKWoiIiIiI+CKFKz9yqKiFRq5ERERERHyNwpUf0Y2ERURERER8l8KVH1HFQBERERER36Vw5Ud0rysREREREd+lcOVHNC1QRERERMR3KVz5kZqRK00LFBERERHxPQpXfiRSI1ciIiIiIj5L4cqPRIZo5EpERERExFcpXPkRrbkSEREREfFdCld+ROFKRERERMR3KVz5kQirpgWKiIiIiPgqhSs/opErERERERHfpXDlR3QTYRERERER36Vw5UdqRq5smhYoIiIiIuJzFK78SGT1yFVFlYPyKruXeyMiIiIiIodTuPIj4dUjV6B1VyIiIiIivkbhyo9YzCbCgiyAwpWIiIiIiK9RuPIzNUUtVI5dRERERMS3KFz5GZVjFxERERHxTQpXfiYyRCNXIiIiIiK+SOHKzxwqx66RKxERERERX6Jw5WcOrblSuBIRERER8SUKV37m0JorTQsUEREREfElCld+RgUtRERERER8k8KVn4lUKXYREREREZ+kcOVnNHIlIiIiIuKbFK78zKFqgRq5EhERERHxJQpXfibCqmqBIiIiIiK+SOHKz2haoIiIiIiIb1K48jMRKmghIiIiIuKTFK78zKE1Vxq5EhERERHxJQpXfiYyxDlyVVHloLzK7uXeiIiIiIhIDYUrPxNuDXA917orERERERHfoXDlZyxmkytgKVyJiIiIiPgOhSs/dKhioIpaiIiIiIj4CoUrP6Ry7CIiIiIivser4WratGn07t2biIgIEhISGDFiBJs2bTrqMf/9738ZMGAAMTExxMTEMHjwYH755Re3NmPHjsVkMrltF1xwwYn8KieVyrGLiIiIiPger4arRYsWMW7cOJYuXcrcuXOprKzk/PPPp7i4+IjHLFy4kKuvvpoFCxawZMkS0tLSOP/888nOznZrd8EFF5Cbm+va3n333RP9dU4alWMXEREREfE9AcducuLMmTPH7fX06dNJSEhgxYoVnH322XUe884777i9fu211/j444+ZP38+1157rWu/1WolKSnJ8532AYdGrhSuRERERER8hU+tuSooKAAgNja23seUlJRQWVlZ65iFCxeSkJBAhw4duO2229i/f/8Rz1FeXo7NZnPbfJlr5KpU0wJFRERERHyFz4Qrh8PBhAkTOPPMM+natWu9j/vHP/5BSkoKgwcPdu274IILeOutt5g/fz7/+te/WLRoEcOGDcNur/umu9OmTSMqKsq1paWlNfr7nEgqaCEiIiIi4nu8Oi3wcOPGjWPdunUsXry43sc89thjvPfeeyxcuJDg4GDX/quuusr1/LTTTqNbt260adOGhQsXMmjQoFrnmTRpEhMnTnS9ttlsPh2wIlXQQkRERETE5/jEyNX48eP54osvWLBgAampqfU65sknn+Sxxx7j22+/pVu3bkdt27p1a5o1a8aWLVvqfN9qtRIZGem2+TKNXImIiIiI+B6vjlwZhsHtt9/OrFmzWLhwIa1atarXcY8//jj//Oc/+eabb+jVq9cx2+/atYv9+/eTnJzc2C77BNfIVblGrkREREREfIVXR67GjRvH22+/zcyZM4mIiCAvL4+8vDxKS0tdba699lomTZrkev2vf/2LBx54gDfeeIP09HTXMUVFRQAUFRVx9913s3TpUrZt28b8+fO59NJLadu2LUOHDj3p3/FE0MiViIiIiIjv8Wq4eumllygoKGDgwIEkJye7tvfff9/VZseOHeTm5rodU1FRweWXX+52zJNPPgmAxWJhzZo1XHLJJbRv354bbriBnj178sMPP2C1Wk/6dzwRVIpdRERERMT3eH1a4LEsXLjQ7fW2bduO2j4kJIRvvvmmEb3yfYdGrjQtUERERETEV/hEQQs5Pq77XGnkSkRERETEZyhc+aGaaYEVVQ7Kq+q+d5eIiIiIiJxcCld+KNx6aDan1l2JiIiIiPgGhSs/ZDGbXAFL4UpERERExDcoXPkpFbUQEREREfEtCld+ylXUolQjVyIiIiIivkDhyk8duteVRq5ERERERHyBwpWfOjQtUCNXIiIiIiK+QOHKT9WMXNk0ciUiIiIi4hMUrvxUpEauRERERER8isKVnzq05krhSkRERETEFyhc+SmVYhcRERER8S0KV35K0wJFRERERHyLwpWfck0LLNfIlYiIiIiIL1C48lMqxS4iIiIi4lsUrvyUClqIiIiIiPgWhSs/pYIWIiIiIiK+ReHKT9WEK5tGrkREREREfILClZ+qmRZYUeWgrNLu5d6IiIiIiIjClZ8Ktwa4nmvdlYiIiIiI9ylc+SmL2eQKWFp3JSIiIiLifQpXfkw3EhYRERER8R0KV35M5dhFRERERHyHwpUfUzl2ERERERHfoXDlxyI0LVBERERExGcoXPmxmmmBNo1ciYiIiIh4ncKVH9PIlYiIiIiI71C48mMqaCEiIiIi4jsUrvyYClqIiIiIiPgOhSs/pvtciYiIiIj4DoUrP6aCFiIiIiIivkPhyo+poIWIiIiIiO9QuPJjhwpaaORKRERERMTbFK78WGSIRq5ERERERHyFwpUfUyl2ERERERHfoXDlx2rWXFXYHZRV2r3cGxERERGRU5vClR8LDwrAZHI+1+iViIiIiIh3KVz5MbPZRHiQbiQsIiIiIuILFK78nMqxi4iIiIj4BoUrP6eiFiIiIiIivkHhys8dGrnStEAREREREW/yariaNm0avXv3JiIigoSEBEaMGMGmTZuOedyHH35Ix44dCQ4O5rTTTuOrr75ye98wDCZPnkxycjIhISEMHjyYzZs3n6iv4VWaFigiIiIi4hu8Gq4WLVrEuHHjWLp0KXPnzqWyspLzzz+f4uLiIx7z008/cfXVV3PDDTewatUqRowYwYgRI1i3bp2rzeOPP86zzz7Lyy+/zM8//0xYWBhDhw6lrKzsZHytk6pmWqBNI1ciIiIiIl5lMgzD8HYnauzdu5eEhAQWLVrE2WefXWebK6+8kuLiYr744gvXvjPOOIPu3bvz8ssvYxgGKSkp3Hnnndx1110AFBQUkJiYyPTp07nqqquO2Q+bzUZUVBQFBQVERkZ65sudIPfNWss7P+/gjkHtmDikvbe7IyIiIiLSpBxPNvCpNVcFBQUAxMbGHrHNkiVLGDx4sNu+oUOHsmTJEgCysrLIy8tzaxMVFUXfvn1dbf6ovLwcm83mtvmLQwUtNHIlIiIiIuJNPhOuHA4HEyZM4Mwzz6Rr165HbJeXl0diYqLbvsTERPLy8lzv1+w7Ups/mjZtGlFRUa4tLS2tMV/lpIoM0ZorERERERFf4DPhaty4caxbt4733nvvpH/2pEmTKCgocG07d+486X1oKI1ciYiIiIj4hgBvdwBg/PjxfPHFF3z//fekpqYetW1SUhK7d+9227d7926SkpJc79fsS05OdmvTvXv3Os9ptVqxWq2N+AbeE6lqgSIiIiIiPsGrI1eGYTB+/HhmzZrFd999R6tWrY55TL9+/Zg/f77bvrlz59KvXz8AWrVqRVJSklsbm83Gzz//7GrTlKgUu4iIiIiIb/DqyNW4ceOYOXMmn376KREREa41UVFRUYSEhABw7bXX0rx5c6ZNmwbAX//6V8455xz+/e9/c9FFF/Hee++xfPlyXn31VQBMJhMTJkzgkUceoV27drRq1YoHHniAlJQURowY4ZXveSJpWqCIiIiIiG/warh66aWXABg4cKDb/jfffJOxY8cCsGPHDszmQwNs/fv3Z+bMmdx///3ce++9tGvXjtmzZ7sVwfj73/9OcXExN998M/n5+Zx11lnMmTOH4ODgE/6dTjaNXImIiIiI+Aafus+Vr/Cn+1xl55dy5mPfEWQx89s/h3m7OyIiIiIiTYrf3udKjl/NyFWF3UFZpd3LvREREREROXUpXPm58KAATCbnc00NFBERERHxHoUrP2c2mwgPqll3paIWIiIiIiLeonDVBKiohYiIiIiI9ylcNQE15dhtGrkSEREREfEahasmQCNXIiIiIiLep3DVBESG6EbCIiIiIiLepnDVBGjkSkRERETE+xSumoCacGVTuBIRERER8RqFK19XUQLbl0Bl6RGb1BS00LRAERERERHvUbjydS/0gTcvgJxVR2yiaYEiIiIiIt6ncOXrkjOcj7uWH7GJRq5ERERERLxP4crXpfZyPu5adsQmkRq5EhERERHxOoUrX9e8OlxlrzhiE00LFBERERHxPoUrX5fSA0xmsGWDLafOJpoWKCIiIiLifQpXvs4aDgmdnc+PsO5KI1ciIiIiIt7XoHC1c+dOdu3a5Xr9yy+/MGHCBF599VWPdUwO07yn8zH7SOGqZuSqCsMwTlavRERERETkMA0KV//3f//HggULAMjLy2PIkCH88ssv3HfffUydOtWjHRQgtbfzcVfd665qRq4q7A7Kqxwnq1ciIiIiInKYBoWrdevW0adPHwA++OADunbtyk8//cQ777zD9OnTPdk/gUMVA3NWgr321L/woABMJudzm9ZdiYiIiIh4RYPCVWVlJVarFYB58+ZxySWXANCxY0dyc3M91ztxatYegiKgsgT2/lrrbbPZRLhV665ERERERLypQeGqS5cuvPzyy/zwww/MnTuXCy64AICcnBzi4uI82kEBzBZofrrz+RHudxV52LorERERERE5+RoUrv71r3/xyiuvMHDgQK6++moyMjIA+Oyzz1zTBcXDXDcTPvq6K5VjFxERERHxjoCGHDRw4ED27duHzWYjJibGtf/mm28mNDTUY52Tw7huJqxy7CIiIiIivqhBI1elpaWUl5e7gtX27dt5+umn2bRpEwkJCR7toFSrGbnauwnKCmq9rRsJi4iIiIh4V4PC1aWXXspbb70FQH5+Pn379uXf//43I0aM4KWXXvJoB6VaeAJEtwAMyF5Z622NXImIiIiIeFeDwtXKlSsZMGAAAB999BGJiYls376dt956i2effdajHZTD1Nzvqo6pgTXhyqZwJSIiIiLiFQ0KVyUlJURERADw7bffMmrUKMxmM2eccQbbt2/3aAflMDXrrnbVFa40LVBERERExJsaFK7atm3L7Nmz2blzJ9988w3nn38+AHv27CEyMtKjHZTDpB4WrgzD7S1NCxQRERER8a4GhavJkydz1113kZ6eTp8+fejXrx/gHMXq0aOHRzsoh0nqBuZAKNkH+e4jhBq5EhERERHxrgaVYr/88ss566yzyM3Ndd3jCmDQoEGMHDnSY52TPwgMhqTTIGelc/QqJt31VqRGrkREREREvKpBI1cASUlJ9OjRg5ycHHbt2gVAnz596Nixo8c6J3VIrXvdVWT1yJVNI1ciIiIiIl7RoHDlcDiYOnUqUVFRtGzZkpYtWxIdHc3DDz+Mw+HwdB/lcEeoGKg1VyIiIiIi3tWgaYH33Xcfr7/+Oo899hhnnnkmAIsXL2bKlCmUlZXxz3/+06OdlMM07+l8zF0NVeUQYAUOX3OlcCUiIiIi4g0NClczZszgtdde45JLLnHt69atG82bN+cvf/mLwtWJFNsaQmKh9ADkrYNUZ9g6NHJViWEYmEwmb/ZSREREROSU06BpgQcOHKhzbVXHjh05cOBAozslR2EyHVp3ddjUwJpwVWk3KK/S1EwRERERkZOtQeEqIyOD559/vtb+559/nm7dujW6U3IMrpsJL3PtCgsKoGawSkUtREREREROvgZNC3z88ce56KKLmDdvnuseV0uWLGHnzp189dVXHu2g1KGOioFms4lwawCFZVUUllWREOGlvomIiIiInKIaNHJ1zjnn8NtvvzFy5Ejy8/PJz89n1KhRrF+/nv/973+e7qP8UU1Ri4NZULzPtTtSRS1ERERERLymQSNXACkpKbUKV6xevZrXX3+dV199tdEdk6MIiYa4drB/M2SvgPZDAfeiFiIiIiIicnI1+CbC4mU197vaVbuohUauREREREROPoUrf1Vdgv3wohaH7nWlkSsRERERkZNN4cpf1VQMzF4JDmfpdY1ciYiIiIh4z3GtuRo1atRR38/Pzz+uD//+++954oknWLFiBbm5ucyaNYsRI0Ycsf3YsWOZMWNGrf2dO3dm/fr1AEyZMoWHHnrI7f0OHTqwcePG4+qbz0vsAgEhUF4A+7dAfHtXQQubwpWIiIiIyEl3XOEqKirqmO9fe+219T5fcXExGRkZXH/99ccMbgDPPPMMjz32mOt1VVUVGRkZXHHFFW7tunTpwrx581yvAwIaXLfDd1kCIaU77FjinBoY3941cmUr1bRAEREREZGT7bhSx5tvvunRDx82bBjDhg2rd/uoqCi3gDd79mwOHjzIdddd59YuICCApKQkj/XTZ6X2coar7OXQY/Rha640ciUiIiIicrL59Zqr119/ncGDB9OyZUu3/Zs3byYlJYXWrVszevRoduzYcdTzlJeXY7PZ3Da/0Nz9ZsIqxS4iIiIi4j1+G65ycnL4+uuvufHGG9329+3bl+nTpzNnzhxeeuklsrKyGDBgAIWFhUc817Rp01yjYlFRUaSlpZ3o7ntGanW42r0eKkpU0EJERERExIv8NlzNmDGD6OjoWgUwhg0bxhVXXEG3bt0YOnQoX331Ffn5+XzwwQdHPNekSZMoKChwbTt37jzBvfeQyOYQkQyGHXIzXQUtCss1ciUiIiIicrL5ZbgyDIM33niDP//5zwQFBR21bXR0NO3bt2fLli1HbGO1WomMjHTb/ILJBM1r7ne1XCNXIiIiIiJe5JfhatGiRWzZsoUbbrjhmG2LiorYunUrycnJJ6FnXlAzNXDXMhW0EBERERHxIq+Gq6KiIjIzM8nMzAQgKyuLzMxMVwGKSZMm1Vna/fXXX6dv37507dq11nt33XUXixYtYtu2bfz000+MHDkSi8XC1VdffUK/i9ek9nY+Zq9wK2hhGIYXOyUiIiIicurx6g2gli9fzrnnnut6PXHiRADGjBnD9OnTyc3NrVXpr6CggI8//phnnnmmznPu2rWLq6++mv379xMfH89ZZ53F0qVLiY+PP3FfxJuSu4PJDLZsIiv3AlBpNyivchAcaPFu30RERERETiFeDVcDBw486gjL9OnTa+2LioqipKTkiMe89957nuia/7CGQ0Jn2L2O0D2rMJkCMAywlVUqXImIiIiInER+ueZK/qB63ZU5ZwXhVhW1EBERERHxBoWrpuCwmwlHqqiFiIiIiIhXKFw1BTVFLXJWEWU1Ac6iFiIiIiIicvIoXDUFzdqDNRIqS+gSkAOArVQjVyIiIiIiJ5PCVVNgNkNKDwBO4zdAI1ciIiIiIiebwlVTUT01sEPVJkBrrkRERERETjaFq6aiumJgq/JfAY1ciYiIiIicbApXTUV1xcD4su1EUIJNI1ciIiIiIieVwlVTER4P0S0xYdDNvFXTAkVERERETjKFq6akempgd9NWTQsUERERETnJFK6akuqpgT3MmzVyJSIiIiJykilcNSXVFQO7m7dSWFbh5c6IiIiIiJxaFK6akqTTcJgDaWayEVaa4+3eiIiIiIicUhSumpLAYMqbdQGgddmvXu6MiIiIiMipReGqiXGk9ASgQ9VGDMPwcm9ERERERE4dCldNjCXNue4qw7SF8iqHl3sjIiIiInLqULhqYqzpfQHobNqGrbDIy70RERERETl1KFw1MabYVhwgAqupivLsNd7ujoiIiIjIKUPhqqkxmdhobu98umuZlzsjIiIiInLqULhqgrYEdQQgKG+Vl3siIiIiInLqULhqgnaEdAYgbF+mdzsiIiIiInIKUbhqgvLCnfe6CiveAcX7vNwbEREREZFTg8JVExQQFs0WR4rzRfYK73ZGREREROQUoXDVBEUEB5JptHW+2LXcu50RERERETlFKFw1QRHBAaxwtHO++PUzMAzvdkhERERE5BSgcNUERQQH8qX9DMrMIbB3I2yZ5+0uiYiIiIg0eQpXTVBkSAA2wvg+4iLnjh+f8W6HREREREROAQpXTVBEcCAAn1kvBZMFtv0AObrnlYiIiIjIiaRw1QRFBAcAsM0eA11HOXf+9LwXeyQiIiIi0vQpXDVBkdXhylZaBf1vd+5cPwvyd3ixVyIiIiIiTZvCVRNUMy2wsKwSkjOg1Tlg2GHpS17umYiIiIhI06Vw1QTVTAssLKvCMAw48w7nGytmQOlBL/ZMRERERKTpUrhqgmpGrqocBmWVDmgzCBI6Q2UxLH/Ty70TEREREWmaFK6aoLAgC2aT83lhWSWYTIfWXv38ClSVe69zIiIiIiJNlMJVE2QymQi3Vhe1KKty7ux6OUQkQ1EerP3Qi70TEREREWmaFK6aKLeiFgABQdD3Vufzn54Dw/BSz0REREREmqYAb3dATozDi1q49LoOvn8S9m6ELfOg3ZBGf85uWxk/Zx3gl6z9/JJ1gL2F5cy4vg/dUqMbfW4REREREX+icNVERbpGrg4LV8FR0HMMLHkefnzmuMOVYRjsOljqFqa27S+p1e7LNbkKVyIiIiJyylG4aqIiQ2pGrird3+h7q/N+V9t+gJxVkNLjiOcwDIOte4v55bAwlVNQ5tbGbILOKZH0SY+jvMrOOz/vYH2OzePfR0RERETE1ylcNVERdY1cAUSnQdfLYO0HzrVXl7/hestWVsmGHBvrsgtYueMgv2QdYF9RhdvhAWYT3VKj6NMqjr6tY+nZMsY1SrZ2V0F1uCrAMAxMJtOJ/ZIiIiIiIj5E4aqJOrTmqrL2m/3Hw9oPMNbP5u2wsSw9EM66nAK21zHFzxpgpkeLaGeYahVLjxbRhAbVfdm0SwwnwGziYEkluQVlpESHePQ7iYiIiIj4MoWrJqomXNnKqsjJL2VddgHrcmxsyClgXbaNJ+1dOMuynoofX+TLqj+7jmseHUKXlEgy0qLp0yqWbqlRWAMs9frM4EALbRPC2ZhXyPocm8KViIiIiJxSvBquvv/+e5544glWrFhBbm4us2bNYsSIEUdsv3DhQs4999xa+3Nzc0lKSnK9fuGFF3jiiSfIy8sjIyOD5557jj59+pyIr+CzaqYFzliyjek/bav1/qvmiznLsp5rAhdgGvgP2rdMo0tKJDFhQY363M4pkWzMK2RDjo0hnRMbdS4REREREX/i1XBVXFxMRkYG119/PaNGjar3cZs2bSIyMtL1OiEhwfX8/fffZ+LEibz88sv07duXp59+mqFDh7Jp0ya3dk1d62ZhgPN2VhaziXYJ4XRtHkWXlEi6No+iU9L58MYXWPes53rrAmg30SOf2yUlik9WZrM+p8Aj5xMRERER8RdeDVfDhg1j2LBhx31cQkIC0dHRdb73n//8h5tuuonrrrsOgJdffpkvv/ySN954g3vuuafOY8rLyykvL3e9ttn8v9rd4E6JzLypL+HWANonRhAcWMfUvv63w+xb4edXoN84CLA2+nO7pDhDryoGioiIiMipxuztDjRE9+7dSU5OZsiQIfz444+u/RUVFaxYsYLBgwe79pnNZgYPHsySJUuOeL5p06YRFRXl2tLS0k5o/08Gs9lE/zbN6JYaXXewAmfVwIhkKMqDtR965HM7V4er7PxS8ksqjtFaRERERKTp8KtwlZyczMsvv8zHH3/Mxx9/TFpaGgMHDmTlypUA7Nu3D7vdTmKi+1qfxMRE8vLyjnjeSZMmUVBQ4Np27tx5Qr+HzwgIct73Cpxl2Q2j0aeMDA4kLdZZyGKDRq9ERERE5BTiV9UCO3ToQIcOHVyv+/fvz9atW3nqqaf43//+1+DzWq1WrNbGT4nzS72ug++fhL0bYfNcaH9+o0/ZJTmKnQdK2ZBro3/bZh7opIiIiIiI7/Orkau69OnThy1btgDQrFkzLBYLu3fvdmuze/dut2qCcpjgKOg5xvn8p2c9ckqtuxIRERGRU5Hfh6vMzEySk5MBCAoKomfPnsyfP9/1vsPhYP78+fTr189bXfR9fW8FkwW2/QA5qxp9ui7Na8KVKgaKiIiIyKnDq9MCi4qKXKNOAFlZWWRmZhIbG0uLFi2YNGkS2dnZvPXWWwA8/fTTtGrVii5dulBWVsZrr73Gd999x7fffus6x8SJExkzZgy9evWiT58+PP300xQXF7uqB0odotOcxS3WfuBce3X5G406XefkKAC27i2mrNJ+5IIaIiIiIiJNiFfD1fLly91uCjxxovNeS2PGjGH69Onk5uayY8cO1/sVFRXceeedZGdnExoaSrdu3Zg3b57bOa688kr27t3L5MmTycvLo3v37syZM6dWkQv5g/63O8PV+tkw6EGIadngUyVGWokLC2J/cQUb8wrpnhbtsW6KiIiIiPgqk2F4oERcE2Oz2YiKiqKgoMDtZsVN3luXwu8L4Yy/wAXTGnWqP7/+Mz9s3sejI0/j//q28Ez/REREREROsuPJBn6/5ko8qP/tzscVM6D0YKNO1SXFOTVQ665ERERE5FShcCWHtBkECV2gshiWv9moU6lioIiIiIicahSu5BCT6dDo1c8vQ0VJg0/VuTpcbcyzYXdo5qmIiIiINH0KV+Ku62UQ1QKKdsOS5xt8mlZxYYQGWSirdPD73iIPdlBERERExDcpXIm7gCAYMsX5fPFTYMtp0GnMZhOdkp2jVxtyNTVQRERERJo+hSuprcsoSOsLlSUwf2rDT6N1VyIiIiJyClG4ktpMJhhaXYp99buQvbJBpzkUrlQxUERERESaPoUrqVtqT+h2lfP5nEnQgNuhdU6uKcduQ7dTExEREZGmTuFKjmzQZAgIgZ1LYf2s4z68fVI4AWYT+SWV5BaUnYAOioiIiIj4DoUrObKo5nDWBOfzuQ9C5fEFJGuAhbYJ4YDWXYmIiIhI06dwJUfX/w6IbA4FO2DpC8d9eJeUmqmBWnclIiIiIk2bwpUcXVAoDHrQ+fyH/0Dh7uM6vLMqBoqIiIjIKULhSo7ttCugeU+oKILvHj6uQ2sqBm5QuBIRERGRJk7hSo7NbIYLHnM+X/U25K6u96E1I1fZ+aXkl1SciN6JiIiIiPgEhSupn7Q+0PUywIA599a7NHtkcCAtYkMBjV6JiIiISNOmcCX1N3gKBATD9sWw8Yt6H9ZF665ERERE5BSgcCX1F90C+t/ufP7t/VBVXq/DOifXhCtVDBQRERGRpkvhSo7PmRMgPAkOboOfX6nXIV2aa+RKRERERJo+hSs5PtZwGDTZ+fz7J6Bo7zEPqbnX1da9RZRV2k9k70REREREvEbhSo5fxtWQnAHlNljwz2M2T4iw0iw8CIcBG/MKT0IHRUREREROPoUrOX6Hl2ZfOQN2rz9qc5PJROfq0SutuxIRERGRpkrhShqmZX/ofCkYDpgz6Zil2Q8VtdC6KxERERFpmhSupOGGTAVLEGQtgt/mHLWpyrGLiIiISFOncCUNF5MO/cY5n397P1RVHLFpTbjamGujyu44CZ0TERERETm5FK6kcc6aCGHxsH8LLHvtiM3S48IIC7JQXuUga1/xSeygiIiIiMjJoXAljRMcCec94Hy+6DEoOVBnM7PZRCetuxIRERGRJkzhShqvxzWQeBqUFcDCaUds1tm17koVA0VERESk6VG4ksYzW+CCR53Pl70OezbW2axm3dWGXI1ciYiIiEjTo3AlntHqbOh4MRh2mHNPnaXZu7judWXDOEbpdhERERERf6NwJZ5z/iNgscLvC2DTV7XebpcYToDZRH5JJTkFZV7ooIiIiIjIiaNwJZ4T2wr6j3c+nzMJKt0DlDXAQtuEcADWZ2vdlYiIiIg0LQpX4llnTYSIFMjfDkueq/X24VMDRURERESaEoUr8SxrOJz/sPP5D/+Bgmy3t1XUQkRERESaKoUr8byul0GLflBZAnMnu73lClcauRIRERGRJkbhSjzPZIJh/wJMsO4j2P6T661O1eEqO7+Ug8UVXuqgiIiIiIjnKVzJiZGcAT3HOp9//Xdw2AGIDA6kRWwooKmBIiIiItK0KFzJiXPeAxAcBXlrYeUM1+6aqYHrc1QxUERERESaDoUrOXHC4uDc+5zP5z8MpQcBrbsSERERkaZJ4UpOrF43QHwnKD0AC6YBKscuIiIiIk2TwpWcWJaA6uIWwLLXYPd618jV1r1FlFbYvdg5ERERERHPUbiSE6/1OdDpEjDs8PU/iA8Poll4EA4DNuZp9EpEREREmgavhqvvv/+e4cOHk5KSgslkYvbs2Udt/8knnzBkyBDi4+OJjIykX79+fPPNN25tpkyZgslkcts6dux4Ar+F1Mv5j0BAMGz7AdPGz+msqYEiIiIi0sR4NVwVFxeTkZHBCy+8UK/233//PUOGDOGrr75ixYoVnHvuuQwfPpxVq1a5tevSpQu5ubmubfHixSei+3I8YlrCmX91Pv/mPjISgwCVYxcRERGRpiPAmx8+bNgwhg0bVu/2Tz/9tNvrRx99lE8//ZTPP/+cHj16uPYHBASQlJTkqW6Kp5w5AVa9AwU7ubjoQ56jv0auRERERKTJ8Os1Vw6Hg8LCQmJjY932b968mZSUFFq3bs3o0aPZsWPHUc9TXl6OzWZz2+QECAqFoY8A0O6312jOXjbm2qiyO7zcMRERERGRxvPrcPXkk09SVFTEn/70J9e+vn37Mn36dObMmcNLL71EVlYWAwYMoLCw8IjnmTZtGlFRUa4tLS3tZHT/1NR5BLQ8C7O9jAes71Je5eD3fcXe7pWIiIiISKP5bbiaOXMmDz30EB988AEJCQmu/cOGDeOKK66gW7duDB06lK+++or8/Hw++OCDI55r0qRJFBQUuLadO3eejK9wajKZnKXZTWYuMC2ln3k963MKvN0rERE5he2xlVFRpVkUItJ4fhmu3nvvPW688UY++OADBg8efNS20dHRtG/fni1bthyxjdVqJTIy0m2TEyipq/PmwsDkgLf4NfuAlzskIiKnqjW78un/2Hfc+NZyDMPwdnfkFGEYBi8s2MKsVbu83RXxML8LV++++y7XXXcd7777LhdddNEx2xcVFbF161aSk5NPQu+k3s69l/LAKDqZd5K8+T1v90ZERE5R7yzdQZXD4Pvf9vLV2jxvd0dOET9t3c8T32zi7g/XUFBa6e3uiAd5NVwVFRWRmZlJZmYmAFlZWWRmZroKUEyaNIlrr73W1X7mzJlce+21/Pvf/6Zv377k5eWRl5dHQcGhaWV33XUXixYtYtu2bfz000+MHDkSi8XC1VdffVK/mxxDaCz7+/wdgFEF0zGK93u5QyIicqopq7Tz5dpc1+tHv/qVskq7F3skp4r3lzmXoFQ5DBZu2uPl3ogneTVcLV++nB49erjKqE+cOJEePXowefJkAHJzc90q/b366qtUVVUxbtw4kpOTXdtf//pXV5tdu3Zx9dVX06FDB/70pz8RFxfH0qVLiY+PP7lfTo4p7pyb2eBoSRTFFM95yNvdERGRU8y3G3ZTVF5F8+gQUqKCyc4v5ZVFv3u7W9LEFZRUMmf9oVHSbzfs9mJvxNNMhiYY12Kz2YiKiqKgoEDrr06wu598iSeK7sHAjOnW7yHpNG93SUREThFj3/yFhZv2csd5bWmXGMHt764iONDMd3cOJCU6xNvdkybqf0u28cCn64kKCaSgtJJwawArHhiMNcDi7a7JERxPNvC7NVfStBgt+/O5/QxMOOCdP8Gqt8GhKRkiInJi7Sks44fN+wAYeXoqF3dLpk96LGWVDqZ9vdHLvZOm7IPlziIWdwxqR0KElaLyKpb+ruJeTYXClXhVl5RI/lk5mr0ByVCYA5+Og5fOhE1fgwZVRUTkBPksMwe7w6BHi2haNQvDZDIxeXhnTCb4fHUOv2Tpl13xvA05NtZmFxBkMTOqR3MGdUoEYO4GFVNpKhSuxKs6J0eSRxxXmJ+C8x+B4GjY+yu8exW8OQx2/OztLoqISBP0ycpsAEadnura17V5FFf1bgHAQ5+vx+7QH/nEsz5Y7ixkMaRzIjFhQZzf2Rmu5m3Yo1sBNBEKV+JVnVOc81a32RwczLgF/roazvobBATDjiXwxvnw3mjYu8nLPRURkaZiY56NDbk2Ai0mhndzv1XLXee3JyI4gPU5NtcvwiKeUF5lZ3amM9Rf0csZ6vu1iSM0yEKerYy12QVHO1z8hMKVeFVEcCAt40IB2JBrg5BoGDwFbl8Jp18LJjNs/AJePAM+ux1sOV7tr4iI+L9Z1aNWgzomEh0a5PZeXLiVCYPbA/DkN5t0DyLxmHkb9pBfUklyVDAD2jmrWAcHWjinvfP5t+tVNbApULgSr+tSPXq1Puewv9hENYdLnoO/LIWOF4PhgJVv4XimB3tnT2L5xizmbdjN7FXZfLEmhy17ijR9Q0REjsnuMJi1yhmuRp7evM421/ZrSZv4MPYXV/Ds/M0ns3vShL1fPRJ6ec9ULGaTa//5XWrWXSlcNQUB3u6ASJeUKL5am8e7v+xk5fZ8CssrKSqrorCsisLyKgrLrqFzVS8mBc6kN78Rn/kigatm8HzVCP5nH0I5zr86hgZZ6JgUQZeUKDqnRNIlJZL2iREEB6q0qYiIOP24ZR97CsuJDg3k3A4JdbYJtJiZPLwLY974hRk/bePqPi1omxB+knsqTUlOfik/bN4LOMPV4c7tkIDFbGLT7kJ27C+hRfWMHvFPClfidd3TogHI2ldM1r7iOtuspD1XVDzIsMBV3G15j9amXdwf+A63Bs/lg5A/8WFBR7IqYlm5I5+VO/Jdx1nMJtrGh9MlJZLO1VuX5CiiQgNPwjcTERFf88lKZxnsSzJSCAo48gSec9rHM7hTAvN+3cPDX2xg+nW9MZlMR2wvcjQfr9iFYcAZrWNpGRfm9l50aBB90mNZ8vt+vt2Qx40DWnupl+IJClfidf3bxPGvy07jQHEl4cEBRAYHEG4NICI4sPrRuYVZAwi0XAz2SbD6XVjwKM0Kc/hL0fP8xQKV0WnkRvdkTUBX5pe2Y+HuEA6WVrFpdyGbdhfySfU0EIDUmBBOax7FNWe05My2zbz47UVE5GQpKq9iznpnyevDqwQeyf0XdWbRb3tZ9Ntevtu4x1U2W+R4OBwGH65whvore6fV2WZI50SW/L6fuRt2K1z5OYUr8TqTycSV1aVv68USAKf/GU67HH75L2yYDTmZBBbupEXhTlowm4sBI7I5ZR3OICusO8vpzI8Ho1ifW8iug6Wu7et1eQzpnMi9F3aiVbOwY32yiIj4sTnr8iirdNC6WRgZqVHHbJ/eLIzrz2rFK4t+5+EvNjCgXfxRR7tE6rI0az87DpQQYQ3ggi7JdbYZ0jmRqV9sYNm2AxwsriAmLKjOduL7FK7EfwWGwJl3OLfyQtj5M2z7Ebb/CNkrMdmyCbF9TGc+pjNwbXgitD6T0uZnsMmawaydYbz9y07mbtjNwk17GNs/nfHntSMqRFMGRUSaopopgaNOb17vKX63n9eOT1Zms21/CW/+mMUt57Q5kV2UJujD5c7rbnj3FEKC6l4HnhYbSsekCDbmFfLdxj1c1vPYI6vimxSupGmwRkDbwc4NoKIEdv1yKGztWgZFu2H9J4Ss/4TuQPfQOP7epiuLChL5Zl8zvl+8nU9XbOeO8ztzVe80Aiy+8ddJwzA4UFxBTGgQZrPm+4uINER2filLft8PwIgedVcJrEu4NYC/D+3A3R+t4bnvtjDy9OYkRASfqG5KE2Mrq+SrtbkA/KlX3VMCa5zfOZGNeYXM3bBb4cqPKVxJ0xQUCq0HOjeAyjLIXl4dthbDzmVQsp+wkkVcCFxYPfpeYbew5atUFsxvQ9vT+tKqS19I7Arh8Se1+4ZhsGZXAV+vy+Prdbls319Cx6QIHr+8G91So09qX0REmoLZq7JdBQVSY46vGttlp6fy9tLtrN5VwBNzNvHEFRknqJfS1HyWmUN5lYP2ieHHnIo6pHMSz363he8376Ws0q5qx35K4UpODYHBkH6Wc+MfUFUBeWsgby3sXge712PsXkdQeSGdTdvpXLkdVn4HK6uPD0+ExC7OoJXYFRI6QbN2zqmJHuJwGKzaeZCv1uYxZ10e2fmlbu9vzCtkxAs/ctPZrfnb4Pb6R1dEpJ4M49C9repTyOKPzGYTD17ShVEv/sSHK3ZxzRktyaiudCtyNB9W39vqT73SjjkVtWvzSJKjgsktKOOnrfs4r6MKqPgjhSs5NQUEQWov51bNZBiQv4OiHav5Zen3lGWvoSPbSTftxly02zmtcOt3h53EBNEtIL4DNGtf/dgB4ttDSEy9umF3GCzbdoCv1+YyZ30eu23lrvdCAi2c1zGBYaclkZEazePfbOLz1Tm8suh3vl2/m39d1o0+rWI99RMREWmy1mYXsGVPEdYAM8O6JjXoHKe3iGFUj+Z8siqbhz5fz8e39VdpdjmqjXk2Vu8qIMBsYmQ9pqKaTCaGdE7krSXb+Xb9boUrP6VwJVLDZIKYloTHtOS8jEvYsqeQh7/8laWbdtLBtIuewdlckVZAB2Mbpn2boPQg5G93bpu/dT9XWMIfQlf1Y0QylQ6Dpb/v5+t1eXy7Po99RRWuw8KtAQzulMAFXZM5p32828LX567uwSUZKdw/ey1Z+4r50ytL+PMZLfnHsI6EWz33P+Utewp5a4lz+su5HeIZ3bcl8RFWj53/SMoq7eTklxIcaCEk0EJIkAVrgFm/vIhIo32y0jlqNbRLEhHBDS9a9I9hHZmzPo+VO/KZnZnNyB5aFyNHVlPIYnCnROLC6/f/ozXhat6ve3A4DK219kMmwzAMb3fC19hsNqKioigoKCAyMtLb3REvW7hpD498+Stb9hQB0D4xnFE9mhNSeZDo4ixiSrKqH38npmQb4eW7j3iuMnMIuY4Ycuwx5BHDbiOWgoBmJKe1pmvHDpzWqSPWqBRnufkjKCitZNpXv/LeMudUg+bRITw66jTOad/wdWFVdgfzft3D/5Zu48ct+93eC7KYubR7Cted2YrOKZ7934NhGKzNLuD9ZTv5LDOHwvKqWm1qglZwgJngoOrgVbOv+nl4cAAXdEliQLtmCmMi4qaiysEZ0+ZzoLiC6df1ZmCHhEad74UFW3jim00kRlr57s6BhHnwj1vSdBx+3b0xtle9R6Eqqhz0fHguheVVfHxbf3q2rN9MGDmxjicbKFzVQeFK/qjS7mDmzzt4at5v5JdUHrVtGKW0MeXQ1pRNW7PzsY0ph3RTHhZTPf7nZjI7R74ikyEixfkYngQmnGvF7M4t90ABK7bmUVlRRhCVpEcF0L5ZEIFGFdjLoarc2dZkhqBwsIY7Hw97XmwKYUVuJYu2lbKrxEIxIZQQzGmtUzmtbUveWV/Oqp0Frq71ax3H9We14ryOCVga8de0/JIKZq/K5v3lu/g11+baHxxoxu4wqLQ37J+lLimR3DawDcO6JjeqfyLSdMzdsJub3lpOfISVJfec1+hKsGWVds5/6nt2HChh3LltuHtoRw/1VJqSr9fmcts7K0mMtPLjP47vurv93VV8vjqHW89pwz3DdH35AoWrRlK4kiPJL6ngjcVZ7DpYXWzCBCZM1AyWmHDOLjTh3GEyUf2eiQCjgubsY0BSJZ3CijAX5YItFwpzqh9zoTAPDLs3vlrdolqwL+EMvihsx0s7UtntcFY6ahkXytj+6VzRK63eUxId1dMh31u2kznr86iocgDOkbGhXZO4qnca/VrHYTabqLQ7KKu0U1bpfCyttFNa4Xwsq96c+xyUVtrZtq+Yj1bsorTS7urfzWe35rLTU1X4Q+QU95d3VvDV2jxuPKsV91/c2SPn/GZ9Hrf8bwVBAWbm/e0cWsQdX/VBafque/MXFmzay18GtuHvFxxfQPpsdQ53vLuKNvFhzL9z4InpoBwXhatGUrgSr3HYoXjfYYGr+rFotzOlWawQYAVLkNvjjoIqPszcwy6bnQoC6doinqv7tyU6IhwMB5QXUVFqY31WNmt+z8ZWcJBwSgmjjOahVbSJMpFgrcRcWey8IXNFEZTm1wp6e0NaMbe0IwsqOvGzoxOGNYo/9U5jbP900mLr/uUit6CUj5bv4oMVO9l54FAFxI5JEVzVO40RPZoTHdr4O9EfLK5gxpJtTP9pm2t0sVm4lRvOasXoM1oQ2Yh1FiLinwpKKun9z3lU2B18dccAj01tNgyDP7/+C4u37OP8zom88ueempIsLnkFZfR/bD4OAxbcNZBWzcKO63hbWSU9H55Lpd3guzvPoXV8+AnqqdSXwlUjKVyJPyqvsvP8d1t4aeFWqhwGUSGBPHBxZ/q1ieOdpdt5b9lODhQ7i2cEWcxc3C2Za/un0/1I5YQrimH7EshaCL8vcpat59A/F3bMrHW04idHF5Y4uhDd4Wz+fHZHeqfHUOUwmP/rHt5ftoNFv+3FUX1YhDWA4d1TuKp3Gqc1jzohv4yUVFTx/rKd/Pf738kpKHN97jX9WnLdmem6+afIKeSdn7dz36x1dEyKYM6Esz167t92FzLsmR+wOwzOaR/PY5edRnKU527PIf6rZl1en/RYPri1X4PO8efXf+aHzfuYNKwjt5zTxsM9lOOlcNVIClfizzbk2Pj7x6tZl22r9V5yVDDXnNGSK3un0ayelYtcSg7Ath+cQStrEezf4vZ2uRHICkc7fgs7nY0VCeSVBVBkBFNMCG2aJzH09LYM7t6GkJBQOAl/4a20O/gsM4eXF21lc3UxkqAAM5f3TOXmAa1JP86/JIqI/7nspZ9Ysf0g913YiZvObu3x87/7yw4e/Gw9FVUOIqwBPHBxZ67olapRrFOYYRgMfHIh2/eX8MTl3biiV1qDzvPWkm1M/nQ9vVrG8NFt/T3cSzleCleNpHAl/q7K7uC/P2Tx1LzfqKhy0K91HGP6t2Rwp8RGL+Z2Kch2hqys76ncsoDA4rz6HWcOqC6qEeFeaMMaDtYoCI6CkGgIjj7C86jjunmzw2Hw3cY9vLhwCyt35Du7YIILT0vm1nPa0LV51PF9bxHxC9v2FTPwyYWYTbB00iASIk/MqPWWPUXc/dFqVlX/+zKwQzzTRmkU61T18+/7ufLVpYQFWVh2/2BCgxpWTTInv5T+j32HyQTL7ht8/H8QFY9SuGokhStpKnILSimvdJz4URrDgP1bKN44n33rvyOy6gDRlnJMFUVQXuRcw1VZ4rnPs1gPBa3gaOdNmyOSICoVIlMgsnn18+YQFFrdRYNl2w7y8qKtfLdxj+tUGalR9EqP5fQWMZzeMlq/EIk0EU/N/Y1n5m/m7PbxvHV9nxPzIbs3QMFO7G0G89ribfx7rvMPWhHB1aNYPTWKdaq584PVfLxyF1f1TuOxy7o16lwXP/cD67JtPH5ZN/7Uu2EjYOIZCleNpHAlcgI47M6QVRO2yougovCw14VQboOyAmcxjbICKMt3f15W4CzQcTxCYiAyFaKau4JXthHHrC0GH281cdARgh0LdszYMZMQGUq3ls04vWUsp7eIpktKFEEBHhrtE5GTwjAMzn5iATsPlPLMVd25tHtzz3/IzmUwYzhUlUJyBpz/T7aEdeeuD9eQuTMfgHM7xDNtVDeSorTW81RQWFZJn3/Op7TS7pF7VD07fzP/mfsbgzsl8NqY3h7qpTSEwlUjKVyJ+CiHwxnE/hi6Sg44S9kX7AJbtnPKoi3b2bahH2WYqMKMAzOGOQDMAVgsFiwBgVgsAc5qjeFJzhGziOTq+5IlH3odkeSc+ugBlXYHOw84R/7S48Iw6x5eIke1bNsBrnh5CWFBFpbfP4SQIA/fkmHvJnhjKJQedN/f4SKqBk3htV8t/OewUawHh3fhstObaxSriXv3lx1M+mQtbeLDmDfxnEb/9/4118awZ37AGmBm1eQhDZ5iKI13PNlA/5VExH+YzRAc6dyiWxy9rWE4w5ctpzpwHR68dh0KYFVldX+UySAIO2AHo5Kap1Qc1ujgtqP3ISi8duCKSIboNGf/o1s4pzWaTDgcBnm2MrL2FfP7vmKy9hazbX8xWfuK2XGgBHt1ycWwIAtdUqLo0jyS05pHcVrzKFrHh+umySKH+WTlLsC5ttLjwaogG/43yhmsmveEy9+En56F5W/Cpi8J2PwNt/a+kSE3jWPiFztZvTOfuz5czVdrc5k26jQST9DaL/G+D5bvBODK3mkeCdIdkyJIjQlh18FSfti8j6Fdkhp9TjnxNHJVB41ciZwiDMM5zdBhB0eV875ejirnCJmjCsNRxc59hazPPsiG7ANszMln534bZsNBMBUkmPJJMh0g0XSQBNNBkjhIovkgSaZ8wqnfGrNScxi5xPO7vRk77M3YZcSzy6h5jMeGc71cSKAFA4OyytrTIkODLHROjqRr8yi6VgeuNvFhnitecpwMw6DKYRDopc+XU1tZpZ3e/5xHYVkVM2/qS/82zTx38tKD8MYw2PsrxLWD67+BsDjne3s3wbcPwOZvnK+Do7APuJv/lg/iP/O3U2F3EFk9ijVKo1hNzubdhQx56nssZhNLJw0iPsIzBSge+nw9b/64jct7pvLkFRkeOaccP00LbCSFKxE5ksKySjJ35rMxt5DcgjLybKXOx4Iy9hSWu0aYQikj0XTQGbxwPiaZDpJoOkCqaR/NTXuJN9Uul/9HVUGRGFEtCIhriRHVkn2ByfxWEc+qoih+2h/K6twySirstY4LDjS7Ba6OSRG0TQg/IdNKCssqWbOrgFU7DpK5M59VO/LZX1xBWJCF6NAgokMDq7cgokOcz2NCg4gKcT663gsNJCokUKFMGuXLNbmMm7mSlKhgFv/jPM9No60shf+NhB1LnFOCb5xb9wj61gXw7f2we53zdUw6Ob0nceuK5qypvkXGoI4JPKpRrCbln19u4L8/ZDGkcyL/vbaXx87709Z9/N9/fyY2LIhl9w3WLAUvUbhqJIUrEWkIu8NgX1F5ddg6FLpcj7ZS9tjKiQsLolV8GO1jLXQJs9Eu8ACppr3EVORgLtgJ+TucW8m+Y3yiCSOyOaXhaewJSOH3qmasLoll6YFwNlbEU0C4e2sTpMWE0j4xgg5J4bRPCKVjDKSH27FWFTsLipQXQpnt0HNzQHVVxkiwRmIPimRbsYU1ex0sz7WzLLuEzXuK8NT/k5hM0D4hgp7pMfRqGUPv9FhSY0L0V36ptxumL2P+xj2MO7cNdw/t6JmT2qvgwzGw8QvnLSOu+wqSuh65vcMOmTPhu4ehaDcARtoZfNTsNu77xUqF3UFUSCCj+7ZgZI/mtEv0zPpM8Y5Ku4MzHp3P/uIK/nttL4Z0TvTYuavsDno+Mo+C0ko+uKUffVrFeuzcUn8KV42kcCUiPqGiGPJ3Qv52OLi9+nEbHMhyPlYWH/3wwEj2ByaT7YihqrwEq72ECEoIN5USQQlhpvJGd7HcCMRGCGXmMBzWSAJCowmNiCEkOomSqNYcCG3DnuB09jiiyS+t5GBJJfklFa7nBSUVrn22sqo6PyMhwkqv9Bh6toylV8sYOieFEViwDXJXQ97aQ5uj6tB6tqgWhz2vfgyJBqC0wk52fgk7D5ay60AJe4sqnFNETSZMOAOeCVP1I66Rjz/uN5kgNCiAqJBAIkMCiQwOqH4MJDIkAGuAh9f6NJDDYfDbnkJiQ4NO2L2ePGXH/hJeX/w7hWVVnNE6jjPbNaN5dP1vj7CvqJy+j87H7jCYN/Ec2iaEH/ugYzEM+PyvsHKG8zYQf/4E0s+q37HlRc71WD8+66wqCNjajmDCvkv5Lu/QtLEuKZGM6N6cS7qnaDTLD32zPo9b/reCZuFWlkw6z+Oj7397P5NZq7K5aUAr7ruos0fPLfWjcNVIClci4vMMA4r3Hha2styDV1E9b+oMlBmBFBJKoRFCIaEUVT+WEIwZOxGUEmkqJoJSIkwlRFJChKn0+PobHAXxHSG+A8R3qn7s6CyPXz0qVWV3sK+ogsydB1m+7SDLtx9kc/Ze2hg76GzeThfTNjqbt9PRtKNBwbDYFEqO0Yzt9jh2GfFkG81c214jmn1EUUHgcZ/3iF850FwdtA4Fr6jq8JUcHUzv9Fi6pUadkBBWUFrJD5v3smDjXhb9tod9RRWYTTCgXTyX90xlSOdEggN9I/wBZOeX8vx3m/lw+S6qHO6/lqTHhdK/bTPOatuMfq3jiAkLOuJ53licxdQvNpCRGsWn4+sZgI5lwaOw6F9gMsMVM6DzJcd/joJs+O4RWP0uYGAEBLO19Z95u6g3724Lpdzh/GXcZIIz2zRjRI/mDO2SSESw565HOXFqRktvOac1k4Z18vj5v1qby1/eWUl6XCgL7hqokXwvULhqJIUrEfF7FSXOka4DWc4y9UFhztLw1upqi9YIsEZhWMPZU2KwKa+Q33Y7t027i9i8u5CSCjsmE7RLCKdHWgw9WkTTvUU07RIisOA47N5ktkP3KKt5bsuBfb/B3o1w4Pcj35/MGlkdtKrDVky6MxzmrYXcNRj7fsNk1F5TVmoEsdFowXpHSzYY6RRFdyLAGoLZtpOw0lyam/aRatpLc9M+mpv20awe69sASiwRFAXEUhgQ63osDIjFFhBLoSUWW0AMNkssRZYoqrBQXGHHVlqJrayq+rGSwiOMwNUlKMBM97Ro+raKpU8r582sw6zHvy7OMAw27yniu417WLBxD8u3H3St/wNnQZTSykM/x8jgAIZnpHB5z1S6p0V77Ze1vIIyXliwhfeW7aDS7uzvOe3j6do8kp+27mfNrgK372EyOUd5zmzTjP5tm9EnPdatGuDw5xazNruAhy7pwpj+6Y3v4LLX4Ms7nc8v+g/0vqFx58vJdK7H2vaDa5cREMLe8A78UpHOt/nNWW20YbvhDL9DOicxonsKZ7eP11pEH7XbVka/afNxGHhutPQPisqrOH3qXCrsDub+7WxNI/UChatGUrgSkVOdw2GQaysjIjiAyMb+9byyDPZvcQatvZsOPR7Y6pzKdyyhcZDUDZK74Ug8je1BbVmaH82yHQWs2H6Q7ftrV2YMDbKQGhNCWkwoqTEhtIoy0SboIGnm/SQ69hBcnI2pYCcU7HSW6S/aA47K4/hSJme/QqKdAdEa4QqvDmsEFZZwysyhFJtCKSKEQiMUmxHMQXswByqt/Jpv4vvt5ewrdv9Mi9lE15RI+rSKpXe6czvSSE1phZ2ftu5jwaY9LNi4l+x899HEtgnhnNshnnM7JtCrZSw5+aV8snIXH6/MdmvbJj6My3umMer05idtStqewjJeWriVd37eQUWVM3j3bxPHxCHt6ZV+aE2JraySn38/wI9b9vHT1n38ttv93nVBFjM9WkRzZttmtIwL5a/vZRJgNvHLfYOJPcoIV71s+BQ+GAMYcM49cO6kxp2vhmHApq/gl1che6XzjxF/YCOcTHsrVhttWONozfbgjpyR0YURPZrTw4thWA4xDINvN+zmqbm/sTGvkJ4tY/j4tv4n7PPGvvkLCzft5e6hHRh3btsT9jlSN4WrRlK4EhE5CaoqnAHr8NB1cJtzjVRyN2egSurmvD/YUX6Z3FNYxqod+VTaHa4wFRsWdHy/gBqGs8x28V5nAYKiPYc9Vj8vrn5evPfII3HHwbAEYQ+OpdAcxR5HONvLwthVHsp+I5IDRHDAiGS/EUFUXDKt01tyWtuWtE2IZNm2A3y3cQ9Lft/vCiYA1gAz/drEcW6HBM7tkECLuNA6P9fhMFj6+34+WrGLr9blusr7n4xpg/uLynn1+9+ZsWSb63N7p8cwcUgH+rWJO+bxe2xl/LR1Pz9u2cePW/aRU1D7PnUeqdaW9QO8PQrsFdDzOrj4qaNegw3mcDj/8JCz0hm0slc4R23ttae95hkxrHa0YUdwBxyJp1EQ1orS0BSCggIJCbQQHGghOMDsfAy0EBxoxhpoOfReoJmokECSIoMVzhrBMAwWbtrLf+b+xtrsAgAirAG8em2vel3DDTXz5x3cO2stGWnRfDruzBP2OVI3hatGUrgSEZEjctihZL8zaJUVVE+PLITyw5/XVF0sPFR50fVYeMSbVx9NlWHmIOGUE0SFEeBcH2YJJDQklIjwMCLDQrEEBkNAEFiCnMUXLIEQYHW+Dgpz3tj6sFG2ElMwP2wv59ONhSzeWU4xIdixEBkcwCXdU7i8ZxoZqVGN/mU8v6SC//7wO2/+uM1164DuadHceX57zmrbrEHnNwyDbftLWLxlHz9t2cdPW/dTWFbJ2zf0pX/bRtzbKm8tvHmh879Xx4vhT2+B+SSuT6uqgD0bnEErZyVG9krYuxFTHYG+3AjkdyOJrUaKc3M4H383kiml7lHIqJBAOidH0jklks7JkXRpHkmb+HBNOzwGwzBYvGUf/5n7G6t25APOEfLrzkznpgGtiQ5t5EjpMeyxldHn0fkA/HzvIBU+OckUrhpJ4UpERE6oylIo3ucst1+8v/pxb6199qK9OIr2EVhVdOxzekgZQRQaIRQZwRQRQllAJCXWBMpCk7FHNMcc1ZyQZi2ISEinWVw8CVHBRxzlspVV8voPWbyxOIvCcucU0NOaRzFxSHsGdoivX6iyVzlHcqpqtjLno9u+cuxVZVTaDYJTujrX7jUkEB7cDq8PcY5UtugPf54FgT7wS2xFMeSupmLHcvZuWkJI/mYii7cTYFQc8ZC95nh2mFPZbmrOVqM5WxzJrC9rRo4jGgfuQSrIYqZ9UrgzdCVH0jklik7JESetoEZReRWb8mxsyLGxIdf5mJ1fRvOYENo0C6NVszBax4fTOj6M9Lgwt3V2J8PS3/fzn29/45dtBwBnsZox/dK5+ezWxIV75mbB9THihR/J3JnPP0d2ZXTfliftc0XhqtEUrkRExKdUlTuDV+nBQ8HCXuEc5bBXVAeNmufVW1W5+/PKksNG1orcR9Iqiho0mlZkBJNrxLHX3IyCwARKQ5KoDEvGFNUcW2AC36zdiam8kHBTKe2jDS7pGEmnWBOmipo+VI/oVRS5j/pVlji/T1UZ1FHQ5JiCoyA5o3rr7nyMbQPmo4zOFO+DN4Y6p+kldHHey6q6fL9Pctid98Pbt9lZPGbfb4eeH+UeeYbJQok1ngOWZuyyx7C5NJLtVTHkGrHkGbHkGnHsIRo7FlrGhdI5OZJOyZEkRwXTLNxKbFgQsWFBxIUHHfdNyQ3DYLetnA25BW5BavuBkuO6V17z6BBax4fR+g/BKyUqxHM3jQZWbD/Af+b+xo9b9gPOAjSj+7bgtoFtSIg4+aH7hQVbeOKbTQzsEM/06/qc9M8/lSlcNZLClYiInHKqKqpDjs0VfEqKDpKdnU3lwZ1gyyawKIfQst1EVewm3Dh5o2kuJgsEVE99DAiunvJodT4GWJ1hbO8mZ6D8o6BwSDrNPXA1aw+WAOf3nTHcufYpKg1umAuRySf963lMyYG6Q9fBbfUKq3bM7DWiyDXiyDVi2W3EUEkAJozqDUwYBJhNhASaCQk0ERxoJjjATEj1Y3CAieAAMxV2BwdLq9hfXMX+kipKKx04MOPAhAMTRvXzkKAA4iJCaBYRTLOIECLCQtltimdLVTxriqNZcyCQ3/cVH/F+eOBcd9iqWRgt40JpEevcUqsfm0eH1Hsd4eqd+fxn7m8s+m0vAIEWE1f2TmPcuW1Jjqr/fdc8bfPuQoY89T1BFjMrJw8hvAGVRaVhFK4aSeFKRETkGCqKMQqyKdm/g6Ld2yjfvxN7/k4sRbkEl+QSVrkfS6AVa1g0Jmv4oVsBuK37+sNW815gaHVgCj4UnCxWZxA6lqoKZ3GU3NWHtry1rpv4ugkIhsSuzjCWtwZCYuH6byC+ved/Xr7AYXeuFbTlgC27+nFX9WOO835chTn1q+J5sgWFY8S0pDKiBQeszckmic1VzVhbEsOK/HC2HqhwlfOvi8kESZHBpMWEklYduNJiQ1whLD7CyoZcG0/N3cy8X3cDzuqdl5+eyu2D2pIaE+osfFNuc4bX0oNQegBK853PHVXOQjcOuzPAGg5nwRLDXr3vj8+rt7B4VzVUIpKO+iMwDINzn1zItv0lvDj6dC487dh/ACgoqWRDro1fq7cte4uIDA4kPS6UFnFhtIwNpWWc82fiS/e+MwyD/cUVFJdX0TIuzNvdUbhqLIUrERGRJsReBfs3uweu3DVQUXioTWAojPkcUhtZadDfORzO9X+u8JXtvFeeowowgck5dlXpMCitNCitdFBS4aC00kFppZ2SSodzq7BTUuEgwGIiITyIhIgg4sODiAsNIMBkuAcM44+vHc51ifk7nDdIt+UAR/l11WTGiGxOeUQLDgSlcNBupaDMIL/cQUGpnYNldkqrTNgx48BMFRYcmLEftpktFrBXEUURsaYiusU56BpjJ9Ruc4anmkDVkGmq9RWWcKhSas1jTCu36az//HID//0hi5E9mvPUld1d+x0Og50HS/g1t2btWiG/5tpq3aLhaJIig2kRF0rL2FDSm4XRojp4tYwNIyrUs+vvbGWV5OaXkVNQSk5+qeu567GgjIoqB12bR/LF7QM8+tkN6q/CVeMoXImIiDRxDofzF/fcTNjzK7QdAi36ertXUpfKMuc96Q5kOac2Hqx5rN4qa9/r7oQKDIWQGOdIZ0i0c7MEgcnsnLpqtjiHylzPzX94bq6uQGlyfq/cNc7wX9ctHoIiIKmrK3Ctc6Qz8sP9hIaEcM+wjmzILuD33L3k5OURWGkjimIiTcVEUUyUqZhISmgeXE5qSDmJgWVEW8ooDogh25zE1qp41pfGssIWzZbyqFqFTg4XFRJIfISVIIuZoIDqzXLoMdDttcnVpqYK5W5bGTn5ZeQWlJKTX0ZR+bFHR00m6JAYwZwJZzfsv5MH+U24+v7773niiSdYsWIFubm5zJo1ixEjRhz1mIULFzJx4kTWr19PWloa999/P2PHjnVr88ILL/DEE0+Ql5dHRkYGzz33HH361H/hn8KViIiIiB8wDOdUR1fo2u4MW3+cpueoOjQlz1H9unqansNhp6y8HLMlkODIZs7gFBp7WICqeV39/ERUkKwogd3rIa96GmvuGufrOu55VkkA2Y44wkylRFFMkKnxo2mGOZDy8FQKgpuz25JMliOBjWVxrCqOZnVR9BFK+xsEUUUw5YRQQYjJ+RhMBcGmCkKq9wdRiYHJbaTQgZlgaxDRYcHEhAUTHR5MbHgIsRGh1WvvQomNCCEwOAziOzT6+zXW8WQDr66EKy4uJiMjg+uvv55Ro0Yds31WVhYXXXQRt956K++88w7z58/nxhtvJDk5maFDhwLw/vvvM3HiRF5++WX69u3L008/zdChQ9m0aRMJCQkn+iuJiIiIyMliMkFEonNr4MijGaj7ltsnUVAopPV2bjXsVc5CJHlrnGErz7kFlhWQbt7tdrhhskBINKbgaGe1zJBoqOu5NcJ5qwG3UcDtmByVBNuyCLZlkQh0Ay6tOXkwVIUmUBEQgdlehrmqFHNVGRZ7KaajTdc8FgMoqt52H6FNXFu4fUXDP8MLfGZaoMlkOubI1T/+8Q++/PJL1q1b59p31VVXkZ+fz5w5cwDo27cvvXv35vnnnwfA4XCQlpbG7bffzj333FOvvmjkSkRERER8jmE416LZsp0FYmqCU1BYw+7tBs6RPFvOoemWhwevA1lQln/sc5gDndMlA4MhMKT6eQgEhDirexrGoYIertHEmkfHH15XHdoX0wqu+7Jh38uD/Gbk6ngtWbKEwYMHu+0bOnQoEyZMAKCiooIVK1YwadIk1/tms5nBgwezZMmSI563vLyc8vJDw642m82zHRcRERERaSyTCWJaOjdPMVsgOs25tapjfVPpQWfYqiiuHZwCqzfLybnhtD/wq3CVl5dHYmKi277ExERsNhulpaUcPHgQu91eZ5uNGzce8bzTpk3joYceOiF9FhERERHxWyExzk3q5Si3Kj91TJo0iYKCAte2c+dOb3dJRERERET8jF+NXCUlJbF7t/uKt927dxMZGUlISAgWiwWLxVJnm6SkI9+YzWq1YrVaT0ifRURERETk1OBXI1f9+vVj/vz5bvvmzp1Lv379AAgKCqJnz55ubRwOB/Pnz3e1ERERERERORG8Gq6KiorIzMwkMzMTcJZaz8zMZMeOHYBzut61117ran/rrbfy+++/8/e//52NGzfy4osv8sEHH/C3v/3N1WbixIn897//ZcaMGfz666/cdtttFBcXc911153U7yYiIiIiIqcWr04LXL58Oeeee67r9cSJEwEYM2YM06dPJzc31xW0AFq1asWXX37J3/72N5555hlSU1N57bXXXPe4ArjyyivZu3cvkydPJi8vj+7duzNnzpxaRS5EREREREQ8yWfuc+VLdJ8rERERERGB48sGfrXmSkRERERExFcpXImIiIiIiHiAwpWIiIiIiIgHKFyJiIiIiIh4gMKViIiIiIiIByhciYiIiIiIeIDClYiIiIiIiAcoXImIiIiIiHhAgLc74Itq7qtss9m83BMREREREfGmmkxQkxGORuGqDoWFhQCkpaV5uSciIiIiIuILCgsLiYqKOmobk1GfCHaKcTgc5OTkEBERgclk8mpfbDYbaWlp7Ny5k8jISK/2RfyPrh9pDF0/0hi6fqShdO1IY5yI68cwDAoLC0lJScFsPvqqKo1c1cFsNpOamurtbriJjIzUPzDSYLp+pDF0/Uhj6PqRhtK1I43h6evnWCNWNVTQQkRERERExAMUrkRERERERDxA4crHWa1WHnzwQaxWq7e7In5I1480hq4faQxdP9JQunakMbx9/aighYiIiIiIiAdo5EpERERERMQDFK5EREREREQ8QOFKRERERETEAxSuREREREREPEDhyse98MILpKenExwcTN++ffnll1+83SXxQd9//z3Dhw8nJSUFk8nE7Nmz3d43DIPJkyeTnJxMSEgIgwcPZvPmzd7prPiUadOm0bt3byIiIkhISGDEiBFs2rTJrU1ZWRnjxo0jLi6O8PBwLrvsMnbv3u2lHosveemll+jWrZvrZp39+vXj66+/dr2va0fq67HHHsNkMjFhwgTXPl0/ciRTpkzBZDK5bR07dnS9781rR+HKh73//vtMnDiRBx98kJUrV5KRkcHQoUPZs2ePt7smPqa4uJiMjAxeeOGFOt9//PHHefbZZ3n55Zf5+eefCQsLY+jQoZSVlZ3knoqvWbRoEePGjWPp0qXMnTuXyspKzj//fIqLi11t/va3v/H555/z4YcfsmjRInJychg1apQXey2+IjU1lccee4wVK1awfPlyzjvvPC699FLWr18P6NqR+lm2bBmvvPIK3bp1c9uv60eOpkuXLuTm5rq2xYsXu97z6rVjiM/q06ePMW7cONdru91upKSkGNOmTfNir8TXAcasWbNcrx0Oh5GUlGQ88cQTrn35+fmG1Wo13n33XS/0UHzZnj17DMBYtGiRYRjOayUwMND48MMPXW1+/fVXAzCWLFnirW6KD4uJiTFee+01XTtSL4WFhUa7du2MuXPnGuecc47x17/+1TAM/dsjR/fggw8aGRkZdb7n7WtHI1c+qqKighUrVjB48GDXPrPZzODBg1myZIkXeyb+Jisri7y8PLdrKSoqir59++pakloKCgoAiI2NBWDFihVUVla6XT8dO3akRYsWun7Ejd1u57333qO4uJh+/frp2pF6GTduHBdddJHbdQL6t0eObfPmzaSkpNC6dWtGjx7Njh07AO9fOwEn/BOkQfbt24fdbicxMdFtf2JiIhs3bvRSr8Qf5eXlAdR5LdW8JwLgcDiYMGECZ555Jl27dgWc109QUBDR0dFubXX9SI21a9fSr18/ysrKCA8PZ9asWXTu3JnMzExdO3JU7733HitXrmTZsmW13tO/PXI0ffv2Zfr06XTo0IHc3FweeughBgwYwLp167x+7ShciYgI4PwL8rp169zmrYscS4cOHcjMzKSgoICPPvqIMWPGsGjRIm93S3zczp07+etf/8rcuXMJDg72dnfEzwwbNsz1vFu3bvTt25eWLVvywQcfEBIS4sWeqaCFz2rWrBkWi6VWZZPdu3eTlJTkpV6JP6q5XnQtydGMHz+eL774ggULFpCamuran5SUREVFBfn5+W7tdf1IjaCgINq2bUvPnj2ZNm0aGRkZPPPMM7p25KhWrFjBnj17OP300wkICCAgIIBFixbx7LPPEhAQQGJioq4fqbfo6Gjat2/Pli1bvP5vj8KVjwoKCqJnz57Mnz/ftc/hcDB//nz69evnxZ6Jv2nVqhVJSUlu15LNZuPnn3/WtSQYhsH48eOZNWsW3333Ha1atXJ7v2fPngQGBrpdP5s2bWLHjh26fqRODoeD8vJyXTtyVIMGDWLt2rVkZma6tl69ejF69GjXc10/Ul9FRUVs3bqV5ORkr//bo2mBPmzixImMGTOGXr160adPH55++mmKi4u57rrrvN018TFFRUVs2bLF9TorK4vMzExiY2Np0aIFEyZM4JFHHqFdu3a0atWKBx54gJSUFEaMGOG9TotPGDduHDNnzuTTTz8lIiLCNR89KiqKkJAQoqKiuOGGG5g4cSKxsbFERkZy++23069fP8444wwv9168bdKkSQwbNowWLVpQWFjIzJkzWbhwId98842uHTmqiIgI19rOGmFhYcTFxbn26/qRI7nrrrsYPnw4LVu2JCcnhwcffBCLxcLVV1/t/X97Tng9QmmU5557zmjRooURFBRk9OnTx1i6dKm3uyQ+aMGCBQZQaxszZoxhGM5y7A888ICRmJhoWK1WY9CgQcamTZu822nxCXVdN4Dx5ptvutqUlpYaf/nLX4yYmBgjNDTUGDlypJGbm+u9TovPuP76642WLVsaQUFBRnx8vDFo0CDj22+/db2va0eOx+Gl2A1D148c2ZVXXmkkJycbQUFBRvPmzY0rr7zS2LJli+t9b147JsMwjBMf4URERERERJo2rbkSERERERHxAIUrERERERERD1C4EhERERER8QCFKxEREREREQ9QuBIREREREfEAhSsREREREREPULgSERERERHxAIUrERERERERD1C4EhERnzZw4EAmTJjg7W64MZlMzJ4929vdEBERH2MyDMPwdidERESO5MCBAwQGBhIREUF6ejoTJkw4aWFrypQpzJ49m8zMTLf9eXl5xMTEYLVaT0o/RETEPwR4uwMiIiJHExsb6/FzVlRUEBQU1ODjk5KSPNgbERFpKjQtUEREfFrNtMCBAweyfft2/va3v2EymTCZTK42ixcvZsCAAYSEhJCWlsYdd9xBcXGx6/309HQefvhhrr32WiIjI7n55psB+Mc//kH79u0JDQ2ldevWPPDAA1RWVgIwffp0HnroIVavXu36vOnTpwO1pwWuXbuW8847j5CQEOLi4rj55pspKipyvT927FhGjBjBk08+SXJyMnFxcYwbN871WQAvvvgi7dq1Izg4mMTERC6//PIT8eMUEZETSOFKRET8wieffEJqaipTp04lNzeX3NxcALZu3coFF1zAZZddxpo1a3j//fdZvHgx48ePdzv+ySefJCMjg1WrVvHAAw8AEBERwfTp09mwYQPPPPMM//3vf3nqqacAuPLKK7nzzjvp0qWL6/OuvPLKWv0qLi5m6NChxMTEsGzZMj788EPmzZtX6/MXLFjA1q1bWbBgATNmzGD69OmusLZ8+XLuuOMOpk6dyqZNm5gzZw5nn322p3+EIiJygmlaoIiI+IXY2FgsFgsRERFu0/KmTZvG6NGjXeuw2rVrx7PPPss555zDSy+9RHBwMADnnXced955p9s577//ftfz9PR07rrrLt577z3+/ve/ExISQnh4OAEBAUedBjhz5kzKysp46623CAsLA+D5559n+PDh/Otf/yIxMRGAmJgYnn/+eSwWCx07duSiiy5i/vz53HTTTezYsYOwsDAuvvhiIiIiaNmyJT169PDIz01ERE4ehSsREfFrq1evZs2aNbzzzjuufYZh4HA4yMrKolOnTgD06tWr1rHvv/8+zz77LFu3bqWoqIiqqioiIyOP6/N//fVXMjIyXMEK4Mwzz8ThcLBp0yZXuOrSpQsWi8XVJjk5mbVr1wIwZMgQWrZsSevWrbngggu44IILGDlyJKGhocfVFxER8S5NCxQREb9WVFTELbfcQmZmpmtbvXo1mzdvpk2bNq52h4cfgCVLljB69GguvPBCvvjiC1atWsV9991HRUXFCelnYGCg22uTyYTD4QCc0xNXrlzJu+++S3JyMpMnTyYjI4P8/PwT0hcRETkxNHIlIiJ+IygoCLvd7rbv9NNPZ8OGDbRt2/a4zvXTTz/RsmVL7rvvPte+7du3H/Pz/qhTp05Mnz6d4uJiV4D78ccfMZvNdOjQod79CQgIYPDgwQwePJgHH3yQ6OhovvvuO0aNGnUc30pERLxJI1ciIuI30tPT+f7778nOzmbfvn2As+LfTz/9xPjx48nMzGTz5s18+umntQpK/FG7du3YsWMH7733Hlu3buXZZ59l1qxZtT4vKyuLzMxM9u3bR3l5ea3zjB49muDgYMaMGcO6detYsGABt99+O3/+859dUwKP5YsvvuDZZ58lMzOT7du389Zbb+FwOI4rnImIiPcpXImIiN+YOnUq27Zto02bNsTHxwPQrVs3Fi1axG+//caAAQPo0aMHkydPJiUl5ajnuuSSS/jb3/7G+PHj6d69Oz/99JOrimCNyy67jAsuuIBzzz2X+Ph43n333VrnCQ0N5ZtvvuHAgQP07t2byy+/nEGDBvH888/X+3tFR0fzySefcN5559GpUydefvll3n33Xbp06VLvc4iIiPeZDMMwvN0JERERERERf6eRKxEREREREQ9QuBIREREREfEAhSsREREREREPULgSERERERHxAIUrERERERERD1C4EhERERER8QCFKxEREREREQ9QuBIREREREfEAhSsREREREREPULgSERERERHxAIUrERERERERD/h/MYsDFWeTfSEAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "\n",
        "#from dataset import BrainTumorDataset\n",
        "#from encoder.encoder_model import get_preprocessing_fn\n",
        "#from Model import UNeT\n",
        "#from model_training import train, evaluate\n",
        "#from utils import save_figure\n",
        "\n",
        "\n",
        "def main(model, preprocessing, device, width, height, learning_rate, weight_decay,  batch_size=8, num_epochs=50):\n",
        "    train_dataset  = BrainTumorDataset(preprocessing_fun=preprocessing, image_size=(height, width))\n",
        "    valida_dataset = BrainTumorDataset(train=False, preprocessing_fun=preprocessing, image_size=(height, width))\n",
        "    print(len(train_dataset), len(valida_dataset))\n",
        "\n",
        "\n",
        "    train_loader  = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader   = data.DataLoader(valida_dataset, batch_size=batch_size, shuffle=False)\n",
        "    print(len(train_loader), len(test_loader))\n",
        "\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    best_model_loss = 1e10\n",
        "    best_model_wts  = copy.deepcopy(model.state_dict())\n",
        "    best_ssimScore = -1\n",
        "\n",
        "    vald_losses = []\n",
        "    train_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model, training_losss, train_ssim,train_psnr = train(model, train_loader, optimizer, device)\n",
        "        model, val_losses, test_ssim ,test_psnr = evaluate(model, test_loader, device)\n",
        "        vald_losses.append(val_losses)\n",
        "        train_losses.append(training_losss)\n",
        "\n",
        "        print(f'epoch {epoch} Training loss {training_losss} training ssim {train_ssim} training psnr {train_psnr} \\\n",
        "                  validation loss {val_losses} and testing ssim {test_ssim} testing_psnr {test_psnr}')\n",
        "\n",
        "        if best_model_loss  > val_losses:\n",
        "            best_model_loss = val_losses\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            best_ssimScore = test_ssim\n",
        "\n",
        "        if epoch %50 ==0 :\n",
        "            learning_rate = learning_rate /10\n",
        "\n",
        "        save_figure(model, test_loader, epoch, device)\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # print(f'Best validation loss {best_model_loss} and best ssim score {best_ssimScore}')\n",
        "    # torch.save(best_model_wts.state_dict(), 'model_weights.pth')\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.title(\"Training and Validation Loss\")\n",
        "    plt.plot(vald_losses,label=\"val\")\n",
        "    plt.plot(train_losses,label=\"train\")\n",
        "    plt.xlabel(\"iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    width = 640\n",
        "    height = 640\n",
        "    encoder_name= 'resnet18'\n",
        "    learning_rate = 5e-4\n",
        "    weight_decay = 1e-5\n",
        "    batch_size = 8\n",
        "\n",
        "    model = UNeT(encoder_name=encoder_name)\n",
        "    model = model.to(device)\n",
        "    preprocessing =  get_preprocessing_fn(encoder_name=encoder_name, pretrained='ssl')\n",
        "\n",
        "    main(model, preprocessing=preprocessing, device=device, width=width, height=height,\n",
        "                       learning_rate=learning_rate, weight_decay=weight_decay, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gp4dviY7X_-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ad2116c-f443-4428-8eb9-9c8b5fdda9b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "433 76\n",
            "55 10\n"
          ]
        }
      ],
      "source": [
        "train_dataset  = BrainTumorDataset(preprocessing_fun=preprocessing, image_size=(height, width))\n",
        "valida_dataset = BrainTumorDataset(train=False, preprocessing_fun=preprocessing, image_size=(height, width))\n",
        "print(len(train_dataset), len(valida_dataset))\n",
        "\n",
        "\n",
        "train_loader  = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader   = data.DataLoader(valida_dataset, batch_size=batch_size, shuffle=False)\n",
        "print(len(train_loader), len(test_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCAPzU3-x_wD"
      },
      "outputs": [],
      "source": [
        "def save_figure1(loader, num_epochs, device, mean = [0.485, 0.456, 0.406], std= [0.229, 0.224, 0.225]):\n",
        "    #model.eval()\n",
        "\n",
        "    if not os.path.exists(os.path.join(os.getcwd(), 'save_results1')):\n",
        "        os.mkdir(os.path.join(os.getcwd(), 'save_results1'))\n",
        "\n",
        "    result_dir = os.path.join(os.getcwd(), 'save_results1')\n",
        "    if not os.path.exists(os.path.join(result_dir, str(num_epoch))):\n",
        "        os.mkdir(os.path.join(result_dir, str(num_epoch)))\n",
        "\n",
        "    save_dir = os.path.join(result_dir, str(num_epoch))\n",
        "    for idx, (x, y) in enumerate(loader):\n",
        "        x = x.to(device).float()\n",
        "        with torch.no_grad():\n",
        "            out = torch.sigmoid(model(x))\n",
        "            torchvision.utils.save_image(out[2,:], f\"{save_dir}/pred_{idx}.png\")\n",
        "            torchvision.utils.save_image(y[2,:],  f\"{save_dir}/target_{idx}.png\")\n",
        "            x = x.permute(0, 2, 3, 1).cpu().numpy()\n",
        "            x = (x *np.array(std)) + np.array(mean)\n",
        "            x = x / x.max()\n",
        "            x = np.transpose(x, (0, 3, 1, 2))\n",
        "            x = torch.from_numpy(x).float().to(device)\n",
        "            torchvision.utils.save_image(x[2,:], f\"{save_dir}/pnoisy_{idx}.png\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4UouRWiXwBL"
      },
      "outputs": [],
      "source": [
        "num_epoch = 50\n",
        "save_figure1(test_loader, num_epoch, device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RGqGdMzLUbfd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}